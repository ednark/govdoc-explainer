<html>
    <head>
        <link rel="stylesheet" type="text/css" href="../../assets/standards.css" />
        <script src="../../assets/standards.js" type="text/javascript"></script>
        
        <script src="../../assets/page_sources.js" type="text/javascript"></script>
        <script src="../../assets/nav.js" type="text/javascript"></script>

        <script src="../../assets/tf.js" type="text/javascript"></script>
        <script src="../../assets/tf-universal-sentence-encoder.js" type="text/javascript"></script>
        <script src="../../assets/page_embedding_search.js" type="text/javascript"></script>
    </head>
    <body>
        <h1>Continuous Monitoring Application</h1>

        
        <div id="nav-menu" class="accordion" role="navigation" aria-label="Page Navigation">
            <div class="accordion-item">
                <button id="nav-menu-toggle" class="accordion-header"><span class="accordion-header-text">Standards</span><span class="accordion-header-icon"></span></button>
                <div class="accordion-content"><ul id="nav-menu-standards"></ul></div>
            </div>
        </div>
        <br /><br />
    

        <div class="accordion">
            <div class="accordion-item">
                <button class="accordion-header" id="source-data-button">Source Data</button>
                <div class="accordion-content" id="source-data-content">
                    <br />
                    <a href="https://www.gao.gov/assets/gao-11-149.pdf">Raw Data</a> | <a href="./Continuous Monitoring Application.txt">Source Text</a>
                    <div id="embed-query">
                        <input type="text" id="embed-query-input"/>
                        <button id="embed-query-button">Search</button>
                        <span id="embed-query-message"></span>
                    </div>
                    <br /><br />
                    <div class="embed-search-results"><div id="chunk-0" class="text-chunk" /><a name="chunk-0"><sup>[0]</sup></a>  
 
 
 
 
 
 
 
 
 
 
 
 
INFORMATION 
SECURITY 
State Has Taken Steps 
to Implement a 
Continuous 
Monitoring 
Application, but Key 
Challenges 
Remain 
 
 
Report to Congressional Requesters
July 2011 
 
GAO-11-149 
 
 
United States Government Accountability Office
GAO 
 
 
 United States Government Accountability Office 
 
Accountability • Integrity • Reliability 
 
Highlights of GAO-11-149, a report to 
congressional requesters 
 
July 2011 
INFORMATION SECURITY 
State Has Taken Steps to Implement a Continuous 
Monitoring Application, but Key Challenges Remain 
Why GAO Did This Study 
The Department of State (State) has 
implemented a custom application 
called iPost and a risk scoring program 
that is intended to provide continuous 
monitoring capabilities of information 
security risk to elements of its 
information technology (IT) 
infrastructure. Continuous monitoring 
can facilitate near real-time risk 
management and represents a 
significant change in the way 
information security activities have 
been conducted in the past. GAO was 
asked to determine (1) the extent to 
which State has identified and 
prioritized risk to the department in its 
risk scoring program; (2) how agency 
officials use iPost information to 
implement security improvements;  
(3) the controls for ensuring the 
timeliness, accuracy, and 
completeness of iPost information; and 
(4) the benefits and challenges 
associated with implementing iPost. To do this, GAO examined program 
documentation and compared it to 
relevant guidance, interviewed and 
surveyed department officials, and 
analyzed iPost data. What GAO Recommends 
GAO recommends the Secretary of 
State direct the Chief Information 
Officer to take a number of actions 
aimed at improving implementation of 
iPost. State agreed with two of GAO’s 
recommendations, partially agreed with 
two, and disagreed with three. GAO 
continues to believe that its 
recommendations are valid and 
appropriate. What GAO Found 
State has developed and implemented a risk scoring program that identifies and 
prioritizes several but not all areas affecting information security risk. Specifically, 
the scope of iPost’s risk scoring program (1) addresses Windows hosts but not 
other IT assets on its major unclassified network; (2) covers a set of 10 scoring 
components that includes many, but not all, information system controls that are 
intended to reduce risk; and (3) assigns a score for each identified security 
weakness, although State could not demonstrate the extent to which scores are 
based on risk factors such as threat, impact, or likelihood of occurrence that are 
specific to its computing environment. As a result, the iPost risk scoring program 
helps to identify, monitor, and prioritize the mitigation of vulnerabilities and 
weaknesses for the areas it covers, but it does not provide a complete view of 
the information security risks to the department.</div><div id="chunk-1" class="text-chunk" /><a name="chunk-1"><sup>[1]</sup></a> State officials reported they used iPost to (1) identify, prioritize, and fix Windows 
vulnerabilities that were reported in iPost and (2) to implement other security 
improvements at their sites. For example, more than half of the 40 survey 
respondents said that assigning a numeric score to each vulnerability identified 
and each component was very or moderately helpful in their efforts to prioritize 
vulnerability mitigation. State has implemented several controls aimed at ensuring the timeliness, 
accuracy, and completeness of iPost information. For example, State employed 
the use of automated tools and collection schedules that support the frequent 
collection of monitoring data, which helps to ensure the timeliness of iPost data. State also relies on users to report when inaccurate and incomplete iPost data 
and scoring are identified, so they may be investigated and corrected as 
appropriate. Notwithstanding these controls, the timeliness, accuracy, and 
completeness of iPost data were not always assured. For example, several 
instances existed where iPost data were not updated as frequently as scheduled, 
inconsistent, or incomplete. As a result, State may not have reasonable 
assurance that data within iPost are accurate and complete with which to make 
risk management decisions. iPost provides many benefits but also poses challenges for the department. iPost 
has resulted in improvements to the department’s information security by 
providing more extensive and timely information on vulnerabilities, while also 
creating an environment where officials are motivated to fix vulnerabilities based 
on department priorities.</div><div id="chunk-2" class="text-chunk" /><a name="chunk-2"><sup>[2]</sup></a> However, State has faced, and will continue to face, 
challenges with the implementation of iPost. These include (1) overcoming 
limitations and technical issues with data collection tools, (2) identifying and 
notifying individuals with responsibility for site-level security, (3) implementing 
configuration management for iPost, (4) adopting a strategy for continuous 
monitoring of controls, and (5) managing stakeholder expectations for continuous 
monitoring activities. View GAO-11-149 or key components. For more information, contact Gregory C. 
Wilshusen at (202) 512-6244 or 
wilshuseng@gao.gov. Page i 
GAO-11-149  State’s iPost Application  
Letter 
 
1 
Background 
2 
Although iPost Does Not Provide a Complete View of Information 
Security Risks, It Helps to Prioritize Vulnerability Mitigation 
Efforts 
10 
State Uses iPost to Identify, Prioritize, and Implement 
Improvements on Windows Hosts 
15 
State Has Implemented Controls Aimed at Ensuring the Timeliness, 
Accuracy, and Completeness of Data in iPost, but Opportunities 
for Improvement Exist 
24 
iPost Provides Many Benefits, but Also Poses Challenges 
27 
Conclusions 
34 
Recommendations for Executive Action 
35 
Agency Comments and Our Evaluation 
36 
Appendix I 
Objectives, Scope, and Methodology 
41 
 
Appendix II 
Examples of Key iPost Screens and Reports 
45 
 
Appendix III 
Comments from the Department of State 
48 
 
Appendix IV 
GAO Contact and Staff Acknowledgments 
58 
 
Tables 
Table 1: Roles and Responsibilities for Information Security at 
State  
8 
Table 2: Scoring Components of State’s Risk Scoring Program 
12 
Table 3: iPost Component Scoring Methodology as of August 2010 
14 
Table 4: Resource Links Available to Users in iPost 
20 
 
Figures 
Figure 1: Usefulness of iPost Information 
17 
Figure 2: Helpfulness of iPost in Accomplishing Site Tasks 
18 
Contents 
 
 
 
 
 
 
 
 
 
 
 
Page ii 
GAO-11-149  State’s iPost Application  
Figure 3: Helpfulness of iPost Features with Prioritizing 
Vulnerability Mitigation 
19 
Figure 4: Usefulness of iPost Resources in Fixing Windows 
Vulnerabilities 
21 
Figure 5: Site Security Improvements Influenced by Using iPost 
22 
Figure 6: Example of an iPost Dashboard Site Summary Screen 
45 
Figure 7: Example of an iPost Site Risk Score Summary Screen 
46 
Figure 8: Example of an iPost Detailed Security Compliance 
Component Screen 
46 
Figure 9: Example of an iPost Risk Score Advisory Report 
47 
 
 
 
 
 
 
 
 
 
 
Abbreviations 
AD 
Active Directory 
CIO 
chief information officer 
CISO 
chief information security officer 
CVSS 
Common Vulnerability Scoring System 
DHS 
Department of Homeland Security 
CAESARS 
Continuous Asset Evaluation, Situational Awareness, and 
Risk Scoring  
FISMA 
Federal Information Security Management Act of 2002  
IT 
information technology 
iPost 
iPost Risk Scoring Program 
NIST 
National Institute of Standards and Technology 
SMS 
Systems Management Server 
SP 
Special Publication 
State 
Department of State 
This is a work of the U.S. government and is not subject to copyright protection in the 
United States. The published product may be reproduced and distributed in its entirety 
without further permission from GAO. However, because this work may contain 
copyrighted images or other material, permission from the copyright holder may be 
necessary if you wish to reproduce this material separately. Page 1 
GAO-11-149  State’s iPost Application  
United States Government Accountability Office
Washington, DC 20548 
July 8, 2011 
The Honorable Joseph I. Lieberman 
Chairman 
Committee on Homeland Security and Governmental Affairs 
United States Senate 
 
The Honorable Thomas R. Carper 
Chairman 
The Honorable Scott Brown 
Ranking Member 
Subcommittee on Federal Financial Management, Government 
Information, Federal Services, and International Security 
Committee on Homeland Security and Governmental Affairs 
United States Senate 
Like other federal agencies, the Department of State (State) is 
increasingly dependent upon information technology (IT) and associated 
services to support functions critical to the department’s mission. At the 
same time, cyber-based threats to federal IT systems and infrastructure 
are evolving and growing and come from a variety of sources including 
foreign nations, criminals, terrorists, and disgruntled insiders. The 
dynamic nature of cyber-based threats and the inevitable changes that 
occur in federal computing environments underscore the need for 
developing new capabilities to provide closer to real-time awareness of 
the security status of federal information systems that support mission 
critical functions, protect assets, and deliver essential services to 
constituents.</div><div id="chunk-3" class="text-chunk" /><a name="chunk-3"><sup>[3]</sup></a> To accomplish such awareness, State has been at the forefront of federal 
efforts in developing and implementing a continuous monitoring 
capability. It has developed and implemented a custom application called 
iPost that is intended to provide continuous monitoring capabilities over 
selected elements of State’s IT environment. Using data collected by 
various automated monitoring and management tools and a scoring 
method based on the premise that higher scores mean higher risk, the 
iPost risk scoring program is intended to provide local administrators and 
enterprise-level management with an improved capability to monitor and 
report on risks and risk mitigation efforts affecting the department’s IT 
infrastructure. Page 2 
GAO-11-149  State’s iPost Application  
You asked us to review the efficiency and effectiveness of the iPost risk 
scoring program. Specifically, our objectives were to determine (1) the 
extent to which State has identified and prioritized risk to the department 
in its risk scoring program; (2) how agency officials use iPost information 
to implement security improvements; (3) the controls for ensuring the 
timeliness, accuracy, and completeness of iPost information; and (4) the 
benefits and challenges associated with implementing iPost. We conducted our review in the Washington, D.C., metropolitan area, 
where we obtained and analyzed program documentation, reports, and 
other artifacts, and interviewed department officials. We compared 
department documentation with State policies and requirements and 
relevant National Institute of Standards and Technology (NIST) guidance 
on risk management and vulnerability scoring. In addition, we developed 
a survey instrument to obtain information from domestic and overseas 
department officials on how they used iPost at their sites and descriptions 
of their experience using it. We also analyzed the timeliness, accuracy, 
and completeness of iPost data for selected State sites. We conducted this performance audit from March 2010 to July 2011 in 
accordance with generally accepted government auditing standards.</div><div id="chunk-4" class="text-chunk" /><a name="chunk-4"><sup>[4]</sup></a> Those standards require that we plan and perform the audit to obtain 
sufficient, appropriate evidence to provide a reasonable basis for our 
findings and conclusions based on our audit objectives. We believe that 
the evidence obtained provides a reasonable basis for our findings and 
conclusions based on our audit objectives. Further details of our 
objectives, scope, and methodology are included in appendix I. Information security is a critical consideration for any organization reliant 
on IT and especially important for government agencies, where 
maintaining the public’s trust is essential. The Federal Information 
Security Management Act of 2002 (FISMA) established a framework 
designed to ensure the effectiveness of security controls over information 
resources that support federal operations and assets. According to 
FISMA, each agency is responsible, among other things, for providing 
information security protections commensurate with the risk and 
magnitude of the harm resulting from unauthorized access, use, 
disclosure, disruption, modification, or destruction of information collected 
or maintained by or on behalf of the agency and information systems 
used or operated by an agency or by a contractor of an agency or other 
organization on behalf of an agency. Consistent with its statutory 
responsibilities under FISMA, in February 2010, NIST issued Special 
Background 
 
  
 
 
 
Page 3 
GAO-11-149  State’s iPost Application  
Publication (SP) 800-37
1 on implementing effective risk management
2 
processes to (1) build information security capabilities into information 
systems through the application of management, operational, and 
technical security controls; (2) maintain awareness of the security state of 
information systems on an ongoing basis though enhanced monitoring 
processes; and (3) provide essential information to senior leaders to 
facilitate system authorization decisions regarding the acceptance of risk 
to organizational operations and assets, individuals, other organizations, 
and the nation arising from the operation and use of information systems. According to NIST guidance these risk management processes: 
 
promote the concept of near real-time risk management and ongoing 
information system authorization through the implementation of robust 
continuous monitoring processes; 
 
encourage the use of automation to provide senior leaders the 
necessary information to make cost-effective, risk-based decisions 
with regard to the organizational information systems supporting their 
core missions and business functions; 
 
integrate information security into the enterprise architecture and 
system development life cycle; 
 
provide emphasis on the selection, implementation, assessment, and 
monitoring of security controls, and the authorization of information 
systems; 
 
link risk management processes at the information system level to risk 
management processes at the organization level through a risk 
executive (function); and 
 
establish responsibility and accountability for security controls 
deployed within organizational information systems and inherited by 
those systems (i.e., common controls). 1NIST, Guide for Applying the Risk Management Framework to Federal Information 
Systems: A Security Life Cycle Approach, Special Publication 800-37 (Gaithersburg, Md. : 
February 2010).</div><div id="chunk-5" class="text-chunk" /><a name="chunk-5"><sup>[5]</sup></a> 2According to NIST, risk management is a process that allows IT managers to balance the 
operational and economic costs of protective measures and achieve gains in mission 
capability and protect the IT systems and data that support their organizations’ missions. Page 4 
GAO-11-149  State’s iPost Application  
Continuous monitoring of security controls employed within or inherited by 
the system is an important aspect of managing risk to information from 
the operation and use of information systems. 3 Conducting a thorough 
point-in-time assessment of the deployed security controls is a necessary 
but not sufficient practice to demonstrate security due diligence. An 
effective organizational information security program also includes a 
rigorous continuous monitoring program integrated into the system 
development life cycle. The objective of continuous monitoring is to 
determine if the set of deployed security controls continue to be effective 
over time in light of the inevitable changes that occur. Such monitoring is 
intended to assist maintaining an ongoing awareness of information 
security, vulnerabilities,
4 and threats
5 to support organizational risk 
management decisions. The monitoring of security controls using 
automated support tools facilitates near real-time risk management. As 
described in the draft NIST SP 800-137,
6 the monitoring process consists 
of the following various steps: 
 
defining a strategy; 
 
establishing measures and metrics; 
 
establishing monitoring and assessment frequencies; 
 
implementing the monitoring program; 
 
analyzing security-related information and reporting findings; 
 
responding with mitigation actions or rejecting, avoiding, transferring, 
or accepting risk; and 
                                                                                                                       
3According to NIST, the term “continuous” in this context means that security controls and 
organizational risks are assessed, analyzed, and reported at a frequency sufficient to 
support risk-based security decisions as needed to adequately protect organization 
information. 4A vulnerability is a weakness in an information system, system security procedures, 
internal controls, or implementation that could be exploited or triggered by a threat source. 5A threat is any circumstance or event with the potential to adversely impact organizational 
operations (including mission, functions, image, or reputation), assets, individuals, other 
organizations, or the nation through an information system via unauthorized access, 
destruction, modification of information, and/or denial of service.</div><div id="chunk-6" class="text-chunk" /><a name="chunk-6"><sup>[6]</sup></a> Threats to information 
and information systems include environmental disruptions, human or machine errors, and 
purposeful attacks. 6NIST, Information Security Continuous Monitoring for Federal Information Systems and 
Organizations (Draft), Special Publication 800-137 (Gaithersburg, Md. : December 2010). Page 5 
GAO-11-149  State’s iPost Application  
 
reviewing and updating the monitoring strategy and program. In its September 2010 report, Continuous Asset Evaluation, Situational 
Awareness, and Risk Scoring (CAESARS) Reference Architecture 
Report,
7 the Department of Homeland Security (DHS) indicates that a key 
aspect of a continuous monitoring process is analyzing security-related 
information, defining and calculating risk, and assigning scores. The 
report notes that risk scoring can provide information at the right level of 
detail so that managers and system administrators can understand (1) the 
state of the IT systems for which they are responsible, (2) the specific 
gaps between actual and desired states of security protections, and (3) 
the numerical value of every remediation action that can be taken to close 
the gaps. This information should help enable responsible managers to 
identify actions that can add value to improving security. The report also 
notes that risk scoring is not a substitute for other essential operational 
and management controls, such as incident response, contingency 
planning, and personnel security. When used in conjunction with other 
sources of information, such as the Federal Information Processing 
Standards 199
8 security categorization and automated asset data 
repository and configuration management tools, risk scoring can be an 
important contributor to an overall risk management strategy. NIST is in the process of developing guidance
9 that extends the 
CAESARS framework provided by DHS.</div><div id="chunk-7" class="text-chunk" /><a name="chunk-7"><sup>[7]</sup></a> NIST’s extension is to provide 
information on an enterprise continuous monitoring technical reference 
architecture to enable organizations to aggregate collected data from 
security tools, analyze the data, perform scoring, enable user queries, 
and provide overall situational awareness. 7Department of Homeland Security, Continuous Asset Evaluation, Situational Awareness, 
and Risk Scoring Reference Architecture Report (CAESARS), No. MP100146 
(Washington, D.C.: September 2010). 8The Federal Information Processing Standards 199, Standards for Security 
Categorization of Federal Information and Information Systems, lays out standards for 
categorizing federal information and information systems as either low impact, moderate 
impact, or high impact according to the potential impact to an agency should events occur 
that jeopardize the information and information systems needed by the organization to 
accomplish its mission. 9NIST,CAESARS Framework Extension: An Enterprise Continuous Monitoring Technical 
Reference Architecture (Draft), Interagency Report 7756 (Gaithersburg, Md. : 
February 2011). Page 6 
GAO-11-149  State’s iPost Application  
NIST has also emphasized the value of planning, scheduling, and 
conducting assessments of controls as part of a continuous monitoring 
program in SP 800-37. This program allows an organization to maintain 
the security authorization of an information system over time in a highly 
dynamic environment of operation with changing threats, vulnerabilities, 
technologies, and missions or business processes. Continuous 
monitoring of security controls using automated support tools facilitates 
near real-time risk management and promotes organizational situational 
awareness with regard to the security state of the information system. State’s key missions are to (1) strive to build and maintain strong bilateral 
and multilateral relationships with other nations and international 
organizations; (2) protect the nation against the transnational dangers 
and enduring threats arising from tyranny, poverty, and disease, global 
terrorism, international crime, and the spread of weapons of mass 
destruction; and (3) combine diplomatic skills and development 
assistance to foster a more democratic and prosperous world integrated 
into the global economy.</div><div id="chunk-8" class="text-chunk" /><a name="chunk-8"><sup>[8]</sup></a> To accomplish its missions, State operates more than 260 embassies, 
consulates, and other posts worldwide. In addition, the department 
operates 6,000 passport facilities nationwide, 17 domestic passport 
agencies, 2 foreign press centers, 1 reception center, 5 offices that 
provide logistics support for overseas operations, 20 security offices, and 
2 financial service centers. State is organized into nine functional 
bureaus: the Bureaus of Administration, Consular Affairs, Diplomatic 
Security, Resource Management, Human Resources, Information 
Resource Management, and Overseas Buildings Operations; the Office of 
the Legal Adviser; and the Foreign Service Institute. Among other things, 
these functional bureaus provide services such as policy guidance, 
program management, and administrative support. In addition, State has 
six regional, or geographic bureaus including the Bureau of African 
Affairs, East Asian and Pacific Affairs, European and Eurasian Affairs, 
Western Hemisphere Affairs, Near Eastern Affairs, and South Asian 
Affairs. These bureaus focus on U.S. foreign policy and relations with 
countries within their geographical areas. State’s IT infrastructure, encompassing its worldwide computer and 
communications networks and services, plays a critical role in supporting 
the department’s missions. This includes OpenNet—the department’s 
global unclassified network that uses Internet protocol to link State’s 
domestic and local area networks abroad. OpenNet serves both foreign 
State Leads U.S. Diplomatic Efforts around 
the World 
State Relies on IT to Support 
Its Mission 
 
  
 
 
 
Page 7 
GAO-11-149  State’s iPost Application  
and domestic locations, has tens of thousands of hosts,
10 and about 5,000 
routers and switches.</div><div id="chunk-9" class="text-chunk" /><a name="chunk-9"><sup>[9]</sup></a> The department budget for IT was approximately 
$1.2 billion for fiscal year 2010. The department’s Foreign Affairs Manual
11 assigns the following roles and 
responsibilities for IT to the Bureau of Information Resource Management 
and Bureau of Diplomatic Security: 
 
The Bureau of Information Resource Management, headed by the 
Chief Information Officer (CIO), is to support the effective and efficient 
creation, collection, processing, transmission, dissemination, storage, 
and disposition of information required to formulate and execute U.S. 
foreign policy and manage the department’s daily operations. To meet 
the challenges of providing information in such an environment, the 
bureau relies on IT to disseminate this information throughout the 
foreign affairs community.  
The Bureau of Diplomatic Security has global responsibilities, with 
protection of people, information, and property. Overseas, the bureau 
implements security programs to ensure the safety of those who work 
in every U.S. diplomatic mission. In the U.S., the bureau protects the 
Secretary of State, the U.S. Ambassador to the United Nations, and 
foreign dignitaries who visit the United States. It also investigates 
passport and visa fraud, conducts personnel security investigations, 
and issues security clearances. Additional IT-relevant functions it 
performs are network monitoring and intrusion detection, incident 
handling and response, and threat analysis. The Foreign Affairs Manual also assigns roles and responsibilities to 
various department officials for information security. These roles and 
responsibilities are summarized in the following table.</div><div id="chunk-10" class="text-chunk" /><a name="chunk-10"><sup>[10]</sup></a> 10A host refers to a computer that is connected to a network. 11The Foreign Affairs Manual outlines the organizational responsibilities and authorities 
assigned to each major component within State. Volume 5 of the manual identifies the 
officials responsible for development, oversight, and implementation of the department’s 
IT program and activities, as well as the guidance, standards, and requirements officials 
are expected to follow when undertaking their responsibilities. Page 8 
GAO-11-149  State’s iPost Application  
Table 1: Roles and Responsibilities for Information Security at State 
Role 
Responsibility 
Chief Information Officer 
Serves as the designated accrediting authoritya for non-Special Compartmented 
Information systems, and ensures the availability of State’s IT systems and 
operations to support the department’s diplomatic, consular, and management 
operations. Chief Information Security Officer (CISO) 
Develops and maintains the department’s information security program, and 
coordinates the design and implementation of processes and practices that 
assess and quantify risk. Information Management Officer/Information 
Systems Officer/Systems Administrator  
Develops and maintains system security plans for all IT systems and major 
applications for which they are responsible and participates in risk assessments 
to periodically reevaluate the sensitivity of the system, risk, and mitigation 
strategies. Information Systems Security Officer 
Ensures systems are configured, operated, maintained, and disposed of in 
accordance with all relevant State security guidelines; plays a leading role in 
introducing an appropriate methodology to help identify, evaluate, and minimize 
risks to all IT systems; and is responsible to the CISO to ensure the IT system 
is configured and maintained securely throughout its lifecycle. Source: State. aA designated accrediting authority or authorizing official is a senior management official or executive 
with the authority to formally authorize the operation of an information system and accept 
responsibility for operating the system at an acceptable level of risk to department operations, assets, 
or individuals. State has developed and implemented a complex, custom-made 
application called iPost to provide an enhanced monitoring capability for 
its extensive and worldwide IT infrastructure.</div><div id="chunk-11" class="text-chunk" /><a name="chunk-11"><sup>[11]</sup></a> The source data for iPost 
come from a variety of enterprise management and monitoring tools 
including Active Directory (AD), Systems Management Server (SMS), and 
diagnostic scanning tools. These tools provide vulnerability data, security 
compliance data, anti-virus signature file data, and other system and 
network data to iPost. The data are posted to an iPost database, 
reformatted and reconciled, and then populated into other iPost 
databases. Data are associated with a “site” or “operational unit,”
12 and 
integrated into a single user interface portal (or dashboard) that facilitates 
                                                                                                                       
12Sites, or operational units, within iPost are either identified based on physical location, 
such as an overseas embassy or domestic facility within the United States, or can be 
grouped by administrative responsibility or function, such as all hosts within a particular 
bureau. State also created virtual “unassigned” sites in iPost for hosts for which 
responsibility was not determined. State Has Implemented 
iPost and Risk Scoring 
Program to Monitor and 
Report on IT Security 
Weaknesses 
 
  
 
 
 
Page 9 
GAO-11-149  State’s iPost Application  
monitoring by department users. The primary users of iPost include local 
and enterprise IT administrators and their management. Designed specifically for State, iPost provides summary and detailed data 
as well as the capability to generate reports based on these data. Summary information provides an overview of the current status of hosts 
at a site, including summary statistics and network activity information. Detailed data on hosts within a site are also available through the 
application navigation.</div><div id="chunk-12" class="text-chunk" /><a name="chunk-12"><sup>[12]</sup></a> For example, when looking at data about a specific 
patch, a user can see which hosts need that patch. Users can select a 
specific host within the scope of their control to view all the current data 
iPost has for that host, such as all identified vulnerabilities. Examples of 
key iPost screens and reports for sites are provided in appendix II. State also developed and incorporated a risk scoring program into iPost 
that is intended to provide a continuous risk monitoring capability over its 
Windows-based hosts on the OpenNet network at domestic and overseas 
locations. The program uses data integrated into iPost from several 
monitoring tools to produce what is intended to be a single holistic view of 
technical vulnerabilities. The objectives of the program are to measure 
risk in multiple areas, motivate administrators to reduce risk, measure 
improvement, and provide a single score for each host, site, and the 
enterprise. Each host and user account is scored in multiple areas known as scoring 
components. The scoring program assigns a score to each vulnerability, 
weakness, or other infrastructure issue identified for the host based on 
the premise that a higher score means higher risk. Thus, the score for a 
host is the total of the scores of all its weaknesses. Scores are then 
aggregated across components to give a total or “raw” risk score for each 
host, site, region, or the enterprise.</div><div id="chunk-13" class="text-chunk" /><a name="chunk-13"><sup>[13]</sup></a> Scores are “normalized” so that small 
and large sites can be equitably compared. 13 Letter grades (“A” through 
“F”), based on normalized scores, are provided to both administrators and 
senior management with the intent of encouraging risk reduction. The 
scoring program also has an “exception” process that aims to 
                                                                                                                       
13The “normalization” of scores involves the calculation of average scores using the 
number of hosts as the denominator. Thus, the average score for an aggregate (i.e., 
component, host, site, or enterprise) is equal to the aggregate raw score divided by the 
aggregate number of hosts or equivalently, the sum of the average scores of its 
components. Implementation of Risk Scoring 
Program Is to Support 
Continuous Risk Monitoring 
 
  
 
 
 
Page 10 
GAO-11-149  State’s iPost Application  
accommodate anomaly situations where the risk cannot be reduced by 
local administrators because of technical or organizational impediments 
beyond local control. In such cases, the risk score is to be transferred to 
the site or operational unit that has responsibility for mitigating the 
weakness and local administrators are left to address only those 
weaknesses within their control. According to a State official, summary data (scores by site and 
component) are permanently retained in a database while detailed data 
were generally retained until replaced by updated data from a recent 
scan. In instances when a host is missed on a scan, the older detailed 
data are kept until they are judged to be too old to be useful. After that, 
the host is scored for nonreporting, and the older data are deleted. The 
official also noted that under a new policy being implemented, detailed 
data will be retained for two to three scans so that users at a site can see 
what changed.</div><div id="chunk-14" class="text-chunk" /><a name="chunk-14"><sup>[14]</sup></a> State has been recognized as a leader in federal efforts to develop and 
implement a continuous risk monitoring capability. In its CAESARS 
reference architecture report, DHS recognized State as a leading federal 
agency and noted that DHS’s proposed target-state reference 
architecture for security posture monitoring and risk scoring is based, in 
part, on the work of State’s security risk scoring program. In addition, in 
2009 the National Security Agency presented an organizational 
achievement award to State’s Site Risk Scoring Program team for 
significantly contributing to the field of information security and the 
security of the nation. The iPost risk scoring program identifies and prioritizes several but not all 
areas affecting information security risk to State’s IT infrastructure. Specifically, the scope of the iPost risk scoring program: 
 
addresses Windows hosts but not other IT assets on the OpenNet 
network, such as routers and switches; 
 
covers a set of 10 scoring components that includes several but not 
all information system controls that are intended to reduce risk; and 
 
assigns a score for each identified security weakness, but the extent 
to which the score reflects risk factors such as the impact and 
likelihood of threat occurrence that are specific to State’s computing 
environment could not be demonstrated. State Has Been Recognized for 
Its iPost Risk Scoring Program 
Although iPost Does 
Not Provide a 
Complete View of 
Information Security 
Risks, It Helps to 
Prioritize 
Vulnerability 
Mitigation Efforts 
 
  
 
 
 
Page 11 
GAO-11-149  State’s iPost Application  
As a result, the iPost risk scoring program helps to identify, monitor, and 
prioritize mitigation of vulnerabilities and weaknesses for the areas it 
covers, but it does not provide a complete view of the information security 
risks to the department. The scope of State’s risk scoring program covers hosts that use Windows 
operating systems, are members of AD, and are attached to the 
department’s OpenNet network. This includes approximately tens of 
thousands workstations and servers at foreign and domestic locations. However, the program’s scope does not include other devices attached to 
the network such as those that use non-Windows operating systems, 
firewalls, routers, switches, mainframes, databases, and intrusion 
detection devices. Vulnerabilities in controls for these devices could 
introduce risk to the Windows hosts and the information the hosts contain 
or process.</div><div id="chunk-15" class="text-chunk" /><a name="chunk-15"><sup>[15]</sup></a> State officials indicated that the focus on Windows hosts for 
risk scoring was due, in part, because of the desire to demonstrate 
success of the risk scoring program before considering other types of 
network devices. Windows servers and workstations also comprised a 
majority of the devices attached to the network and the availability of 
Microsoft tools such as AD and SMS and other enterprise management 
tools facilitated the collection of source data from Windows hosts. State 
officials indicated they were considering expanding the program to 
include scoring other devices on OpenNet. In applying the risk management framework to federal information 
systems, agencies select, tailor, and supplement a set of baseline 
security controls using the procedures and catalogue of security controls 
identified in NIST SP 800-53, rev. 3. The effective implementation of 
these controls is intended to cost-effectively mitigate risk while complying 
with security requirements defined by applicable laws, directives, policies, 
standards, and regulations. To ensure that the set of deployed security 
controls continues to be effective over time in light of inevitable changes 
that occur, NIST SP 800-37 states that agencies should assess and 
monitor a subset of security controls including technical, management, 
and operational controls on an ongoing basis during continuous 
monitoring. Using data integrated into iPost from multiple monitoring tools that identify 
and assess the status of security-related attributes and control settings, 
the iPost risk scoring program supports a capability to assess and monitor 
iPost Risk Scoring 
Program Only Addresses 
Windows Host Computers 
on the OpenNet Network 
iPost’s Risk Scoring 
Program Addresses 
Several but Not All 
Information System 
Controls 
 
  
 
 
 
Page 12 
GAO-11-149  State’s iPost Application  
a subset of the security controls including technical and operational 
controls on an ongoing basis. The program is built on a set of 10 scoring 
components, each of which, according to iPost documentation, 
represents an area of risk for which measurement data were readily 
available. The program addresses vulnerabilities, security weaknesses, 
and other control issues affecting risk to the Windows hosts.</div><div id="chunk-16" class="text-chunk" /><a name="chunk-16"><sup>[16]</sup></a> The 10 
scoring components in iPost are described in the following table. Table 2: Scoring Components of State’s Risk Scoring Program 
Scoring component 
What is scored 
Source 
1. Vulnerability 
Vulnerabilities detected on a host 
Scanning tool 
2. Patch 
Incompletely installed or uninstalled patches required by a host 
SMS 
3. Security compliance 
Failures of a host to use required security settings 
Scanning tool 
4. Anti-virus 
Out-of-date anti-virus signature file 
SMS 
5. Standard operating environment 
compliance 
Incomplete/invalid installations of any product in the Standard 
Operating Environment suite, of which there were 19 products 
SMS 
6. AD users 
User account password ages exceeding 60-day threshold (scores 
each user account, not each host) 
AD 
7. AD computers 
Computer account password ages exceeding 30-day threshold 
AD 
8. SMS reporting 
SMS client agent on host is not reporting all expected information 
and the incomplete reporting is due to specific type of errors 
SMS 
9.</div><div id="chunk-17" class="text-chunk" /><a name="chunk-17"><sup>[17]</sup></a> Vulnerability reporting 
Hosts that miss two consecutive vulnerability scans 
Scanning tool 
10. Security compliance reporting 
Hosts that miss two consecutive security compliance scans 
Scanning tool 
Source: GAO analysis of State data. Although iPost provides a capability to monitor several types of security 
controls on an ongoing basis, it did not address other controls intended to 
reduce risk to Windows hosts, thereby providing an incomplete view of 
such risk. These controls include physical and environmental protection, 
contingency planning, and personnel security. Vulnerabilities in these 
controls could introduce risk to the department’s Windows hosts on 
OpenNet. State officials recognized that these controls and associated 
vulnerabilities were not addressed in iPost and stated that when they 
were first developing iPost, they focused on controls and vulnerabilities 
that could be monitored with existing automated tools such as a scanning 
tool, AD, and SMS since these could be implemented immediately. State 
officials believed this approach allowed them to develop a continuous 
monitoring application in the time frame they did with the limited 
resources available. Department officials also advised that the scoring 
program is intended to be scalable to address additional controls and they 
may add other control areas in the future. Page 13 
GAO-11-149  State’s iPost Application  
According to NIST SP 800-37, risk is a measure of the extent to which an 
entity is threatened by a potential circumstance or event, and typically a 
function of: (1) the adverse impacts
14 that would arise if the circumstance 
or event occurs and (2) the likelihood of occurrence. In information 
assurance risk analysis, the likelihood of occurrence is a weighted factor 
based on a subjective analysis of the probability that a given threat is 
capable of exploiting a given vulnerability.</div><div id="chunk-18" class="text-chunk" /><a name="chunk-18"><sup>[18]</sup></a> According to iPost 
documentation, a key objective of the risk scoring program is to measure 
risk in multiple areas. State could not demonstrate the extent to which it considered factors 
relating to threat, impact, and likelihood of occurrence in assigning risk 
scores for security weaknesses. In developing the scoring methods for 
the 10 scoring components, the department utilized a working group 
comprised of staff from the Bureaus of Information Resource 
Management and Diplomatic Security. While documentation was limited 
to descriptions of the certain scoring calculations assigned to each 
component, State officials explained that working groups comprised of 
staff from the Bureaus of Information Resource Management and 
Diplomatic Security had discussions to determine a range of scores for 
each component. State officials explained that the premise for the scoring 
method was the greater the risk, the higher the score, and therefore, the 
greater the priority for mitigation. However, minutes of the working 
groups’ meetings and other documents did not show the extent to which 
threats, the potential impacts of the threats, and likelihood of occurrence 
were considered in developing the risk scores and State officials 
acknowledged these factors were not fully considered. Table 3 provides a 
description of how State calculates a score for each component. 14Impact is the magnitude of harm that can be expected to result from the consequences 
of unauthorized disclosure, modification, or destruction of information or loss of 
information or information system availability. State Could Not 
Demonstrate the Extent to 
Which iPost Assigns 
Scores for Security 
Weaknesses Based on Risk 
Factors Specific to Its 
Computing Environment 
 
  
 
 
 
Page 14 
GAO-11-149  State’s iPost Application  
Table 3: iPost Component Scoring Methodology as of August 2010 
Component 
How score is calculated for a host 
1. Vulnerability 
Sum of vulnerability scores of all detected vulnerabilities. Scores for individual 
vulnerabilities range from 0.01 and .1 for the lowest-risk vulnerability to 10 for the highest-
risk vulnerability.</div><div id="chunk-19" class="text-chunk" /><a name="chunk-19"><sup>[19]</sup></a> 2. Patch 
Sum of patch scores of all incompletely installed patches. Each patch is assigned a score 
based on its risk level: low = 3, medium = 6, high = 9, and critical = 10. 3. Security compliance 
Sum of all scores of all failed security compliance checks. According to one screen, the 
scores can range from .43 to .9 for each instance of security noncompliance.a 
4. Anti-virus 
After a grace period of 6 days, a score of 6 per day is assigned to a host with an old anti-
virus signature file. 5. Standard operating environment 
compliance 
Score of 5 assigned for each missing or unapproved version of a standard application. 6.</div><div id="chunk-20" class="text-chunk" /><a name="chunk-20"><sup>[20]</sup></a> AD users 
Score of 1 assigned for each day an account that does not require a smart-card, and are 
not disabled or expired, and whose password age exceeds 60 days. Accounts that have 
no date in AD for the last password reset are assigned a fixed score of 200. If the 
password is set to never expire, an additional score of 5 is assigned. 7. AD computers 
Score of 1 assigned for each day password age exceeds 35 days.b 
8. SMS reporting 
One hundred plus 10 for each day since last day agent correctly reported. Before scoring 
begins, there is a grace period that varies from 5 to 30 days, depending on the error 
conditions detected. 9. Vulnerability reporting 
After a host has not been scanned in 15 consecutive days, a score of 5 is assigned, then 
increased at the rate of 1 for each additional 7 days. 10.</div><div id="chunk-21" class="text-chunk" /><a name="chunk-21"><sup>[21]</sup></a> Security compliance reporting 
After a host has not been scanned for 30 consecutive days, a score of 5 is assigned, then 
increased at the rate of 1 for each additional 15 days. Source: GAO analysis of iPost documentation. Note: Numeric scores reflected in the table were displayed in iPost as of August 31, 2010. 
aDocumentation provided by State showed different ranges of scores. For example, another screen 
displayed in iPost indicated that the scores for security compliance can range from .862 to 4.31. The 
iPost risk scoring methodology guide dated August 2010 indicates security compliance scores range 
from .006 to .8. 
bThe iPost risk scoring methodology guide dated August 2010 indicates a score is assigned for each 
day the password age exceeds 30 days. The methodology used to assign scores for the vulnerability component 
illustrates the limits that risk factors such as the impact and likelihood of 
threats specific to State’s environment were considered. Each 
vulnerability is initially assigned a score according to the Common 
Vulnerability Scoring System (CVSS). According to NIST guidance,
15 
                                                                                                                       
15NIST, The Common Vulnerability Scoring System (CVSS) and Its Applicability to Federal 
Agency Systems, Interagency Report 7435 (Gaithersburg, Md. : August 2007). Page 15 
GAO-11-149  State’s iPost Application  
agencies can use the CVSS base scores
16 stored in the National 
Vulnerability Database to quickly determine the severity of identified 
vulnerabilities.</div><div id="chunk-22" class="text-chunk" /><a name="chunk-22"><sup>[22]</sup></a> Although not required, agencies can then refine base 
scores by assigning values to the temporal
17 and environmental
18 metrics 
in order to provide additional contextual information that more accurately 
reflects the risk to their unique environment. However, State did not refine 
the base scores to reflect the unique characteristics of its environment. Instead, it applied a mathematical formula to the base scores to provide 
greater separation between the scores for higher-risk vulnerabilities and 
the scores for lower-risk vulnerabilities. As a result, the scores may not 
fully or accurately reflect the risks to State’s OpenNet network. Although 
the iPost risk scoring program does not provide a complete view of the 
information security risks to the department, it helps to identify, monitor, 
and prioritize mitigation of vulnerabilities and weaknesses associated with 
Windows hosts on the OpenNet network. State officials surveyed responded that they used iPost to (1) identify, 
prioritize, and fix security weaknesses and vulnerabilities on Windows 
devices and (2) implement other security improvements at their sites. For 
example, at least half of the respondents said that assigning a numeric 
score to each vulnerability identified and each component was very 
helpful with prioritizing their efforts to prioritize the mitigation of Windows 
vulnerabilities. State officials stated that iPost was particularly helpful 
because prior to iPost, officials did not have access to tools with these 
capabilities. However, State officials did not use iPost results to update 
key security documents related to the assessment and authorization of 
the OpenNet network. 16The base group of metrics reflects the intrinsic and fundamental characteristics of a 
vulnerability that are constant over time and across user environments, such as how the 
vulnerability is exploited (locally or remotely) or how complex the attack must be to exploit 
the vulnerability once system access has been gained—factors that contribute to the 
likelihood of occurrence.</div><div id="chunk-23" class="text-chunk" /><a name="chunk-23"><sup>[23]</sup></a> The base group metrics also measure the impact to 
confidentiality, integrity, and availability of a successfully exploited vulnerability. 17The temporal group of metrics reflects the characteristics of a vulnerability that change 
over time, including the current state of the exploit techniques or code availability, and 
whether remediation is available to fix the vulnerability. 18The environmental group of metrics provides the score actually needed for risk 
prioritization as it pertains to the user’s environment. The environmental metrics are 
specified by users because users are best able to assess the potential impact of a 
vulnerability within their own environments. State Uses iPost to 
Identify, Prioritize, 
and Implement 
Improvements on 
Windows Hosts 
 
  
 
 
 
Page 16 
GAO-11-149  State’s iPost Application  
State officials reported they used iPost to help them to: (1) identify 
Windows vulnerabilities on the devices for which they were responsible, 
(2) prioritize the mitigation of vulnerabilities identified, and (3) fix the 
vulnerability and confirm mitigation was successfully implemented. Specifically, as part of their duties, State officials indicated they reviewed 
iPost regularly to see the results of the automated scanning of devices at 
their sites to see what vulnerabilities had been identified. In particular, 14 
of 40 survey respondents stated that they viewed the information in iPost 
at least once per day, 17 viewed information in iPost at least once a 
week, 3 viewed information at least once a month, and 4 respondents 
viewed information less than once per month. In addition, State officials 
we interviewed indicated they reviewed iPost information on a daily basis, 
with one official stating it was his first task in the morning. Of the information available in iPost, State officials surveyed noted that 
some screens and information were particularly useful at their sites. Specifically, the majority of the 40 survey respondents reported that the 
site summary screen, site score summary screen, level of detailed 
information on each component, and site reports were very or moderately 
useful (see fig.</div><div id="chunk-24" class="text-chunk" /><a name="chunk-24"><sup>[24]</sup></a> 1). These screens show site and host identifying 
information, statistical data, and graphical representations of the site’s 
risk scores, host computers, accounts, and identified weaknesses for 
each of the 10 components. Appendix II shows sample screens 
containing this information. State Officials Primarily 
Identify, Prioritize, and Fix 
Weaknesses on Windows 
Hosts 
 
  
 
 
 
Page 17 
GAO-11-149  State’s iPost Application  
Figure 1: Usefulness of iPost Information 
The majority of State officials surveyed also indicated that iPost was very 
helpful in identifying Windows vulnerabilities. In particular, the majority of 
the 40 survey respondents indicated that iPost was very or moderately 
helpful in identifying vulnerabilities on devices, providing automated 
scanning of devices onsite for vulnerabilities, and reviewing identified 
vulnerabilities on devices (see fig. 2). 0
5
10
15
20
25
Don't know/
Have never used
Not at all
useful
Slightly
useful
Moderately
useful
Very
useful
Number of respondents
Usefulness of iPost information
Source: GAO survey of State officials. Site summary screen
Site score summary screen
Level of detailed information on each component of the score
Site reports
 
  
 
 
 
Page 18 
GAO-11-149  State’s iPost Application  
Figure 2: Helpfulness of iPost in Accomplishing Site Tasks 
Furthermore, survey respondents and State officials we interviewed also 
reported being able to identify additional site vulnerabilities at their sites 
not scored in iPost. For example, one official we spoke to said she would 
receive incident notices and would use iPost to obtain more information 
about the incident. Another official noted that iPost helped identify users 
who were utilizing the most bandwidth on the network.</div><div id="chunk-25" class="text-chunk" /><a name="chunk-25"><sup>[25]</sup></a> Generally, State 
officials concluded that iPost was particularly helpful because (1) it 
provided several officials access to tools with these capabilities they did 
not have prior to its use and (2) it streamlined the number of software 
utility and scanning tools officials could use, making the monitoring 
process more efficient and effective. State officials reported that the iPost features helped them prioritize the 
mitigation of vulnerabilities at their sites. Most survey respondents 
indicated that iPost was very or moderately helpful with prioritizing the 
mitigation of Windows vulnerabilities. For example, more than half of the 
Risk Scoring Features Help 
Officials Prioritize Vulnerability 
Mitigation 
0
5
10
15
20
25
30
Don't know/
Not applicable
Not at all
helpful
Slightly
helpful
Moderately
helpful
Very
helpful
Number of respondents
Helpfulness in accomplishing site tasks
Source: GAO survey of State officials. Identifying vulnerabilities on devices
Automating vulnerability scanning activities for me
Reviewing identifed vulerabilities on devices
 
  
 
 
 
Page 19 
GAO-11-149  State’s iPost Application  
40 respondents said that assigning a numeric score to each vulnerability 
identified and each component was very or moderately helpful in their 
efforts to prioritize vulnerability mitigation. In addition, over half of the 40 
respondents felt that assigning letter grades to sites was very or 
moderately helpful in prioritization efforts, though 10 respondents felt this 
was only slightly helpful, and 4 respondents felt this was not helpful at all. Of the features presented in iPost that assist in prioritization, responses 
were mixed regarding how helpful ranking of sites in comparison to other 
sites was for prioritizing vulnerability mitigation, with 22 respondents 
reporting it was very or moderately helpful, 9 slightly helpful, and 7 not at 
all helpful. Figure 3 provides details of survey responses. Figure 3: Helpfulness of iPost Features with Prioritizing Vulnerability Mitigation 
 
0
5
10
15
20
25
Don't know/
Not applicable
Not at all
helpful
Slightly
helpful
Moderately
helpful
Very
helpful
Number of respondents
Helpfulness of iPost features
Source: GAO survey of State officials. Assigning a numeric score to each vulnerability identified
Assigning a numeric score to each component
Assigning my site a letter grade
Ranking my site in comparison to other sites
 
  
 
 
 
Page 20 
GAO-11-149  State’s iPost Application  
State officials we interviewed also indicated that iPost assisted them in 
prioritizing vulnerability mitigation.</div><div id="chunk-26" class="text-chunk" /><a name="chunk-26"><sup>[26]</sup></a> In particular, they found that scoring 
the vulnerabilities helped them to identify which ones were necessary to 
fix first. In regards to the letter grades and ranking, a State official told us 
the letter grades were useful because they aided him in deciding whether 
he should fix vulnerabilities identified in iPost (if he/she had a grade lower 
than an A) or focus on other activities. The iPost dashboard also provides links to available resources that users 
can utilize to fix identified vulnerabilities. Table 4 provides an overview of 
available resource links located in iPost. Table 4: Resource Links Available to Users in iPost 
Resource 
Description 
Patch management Web site 
Facilitates the management, installation, and monitoring of Windows operating system 
patches 
SMS post admin tool 
Allows an SMS Administrator to accomplish tasks required to administer an SMS system. IT Change Control Board baseline 
Includes a list of approved hardware and software that can be used on department 
systems that has been approved by the IT Change Control Board, which manages and 
approves changes to the department’s IT infrastructure 
Site Risk Scoring Toolkit 
Provides online reference documents that iPost users can utilize for evaluating the site risk 
data and scores located in iPost 
IT Asset baseline 
Maintains the department’s IT asset inventory 
Diplomatic Security configuration guides 
Documents the required configuration settings that should be in place for various operating 
systems 
IT Service Center 
Provides technical support for department users on IT-related issues 
Source: GAO analysis of State documents. State officials reported they used available resources linked in iPost to 
help them fix vulnerabilities at their sites and confirm those fixes were 
successfully implemented. In particular, the majority of survey 
respondents reported that the patch management Web site, the SMS post 
admin tool, and the IT Change Control Board baseline were very or 
moderately useful in helping them to fix vulnerabilities at their site. Over 
half of the 40 respondents stated that the Site Risk Scoring Toolkit (26), 
IT Asset baseline (25), and the Diplomatic Security configuration guides 
(24) were very or moderately useful in helping them to fix vulnerabilities at 
their site. However, officials also reported they had never used some of 
the resources available in iPost (see fig.</div><div id="chunk-27" class="text-chunk" /><a name="chunk-27"><sup>[27]</sup></a> 4). iPost Resources Help Officials 
Fix Vulnerabilities and Confirm 
Successful Implementation 
 
  
 
 
 
Page 21 
GAO-11-149  State’s iPost Application  
Figure 4: Usefulness of iPost Resources in Fixing Windows Vulnerabilities 
In addition, State officials mentioned they utilized iPost to confirm that 
fixes they made to identified vulnerabilities were successfully 
implemented. In particular, survey respondents reported they either 
waited for the next automated scan results to be posted in iPost (28 of 31 
respondents) or e-mailed headquarters in Washington, D.C., to run 
another scan to see that the fix was implemented (9 of 31 respondents). Regarding the helpfulness of iPost in verifying vulnerability fixes were 
successfully implemented, survey respondents found iPost to be very 
helpful (17 respondents), moderately helpful (12 respondents), slightly 
helpful (5 respondents) or not at all helpful (2 respondents). 0
5
10
15
20
25
30
Don't know/
Have never used
Not at all
useful
Slightly
useful
Moderately
useful
Very
useful
Number of respondents
Usefulness of iPost resources in fixing windows vulnerablilities
Source: GAO survey of State officials. Patch management
Systems Management Server Post Admin Tool
IT CCB baseline
Site Risk Scoring Toolkit
IT Asset baseline
Diplomatic Security configuration guides
IT Service Center
 
  
 
 
 
Page 22 
GAO-11-149  State’s iPost Application  
State officials surveyed reported that using iPost also influenced them to 
make other security improvements at their sites. For example, of the 24 
respondents who reported updating AD at their site, 17 respondents 
reported they were influenced by using iPost to do so, and of the 20 
respondents that reported changing how patches were rolled out, 16 
reported that iPost influenced them in making this change. In addition, 
several respondents reported making security improvements in 
configurations of servers, site security policies, security training, and 
network architecture based in part on their use of iPost (see fig. 5). Figure 5: Site Security Improvements Influenced by Using iPost 
 
 
Using iPost Influenced 
Officials to Implement 
Other Security 
Improvements at Their 
Sites 
0
6
12
18
24
Changed network architecture
(i.e. switches, firewalls)
Changed security
training given to users
Changed security
training given to IT staff
Changed site
security policies
Changed standardized
configuration of servers
Changed standardized
configurations of desktops
Changed how patches
were rolled out
Updated Active
Directory
Number
Site security improvements
Number of respondents making a security improvement
Number of respondents making a security improvement who were influenced by iPost
Source: GAO survey of State officials.</div><div id="chunk-28" class="text-chunk" /><a name="chunk-28"><sup>[28]</sup></a> Page 23 
GAO-11-149  State’s iPost Application  
For example, one survey respondent reported that since the desktops 
shipped to the site had obsolete software on the standardized baseline 
image, he/she removed the old software before deploying the 
workstations at the site. Another survey respondent reported that he/she 
looked at iPost to see whether deployed patches needed to be pushed 
out again or if they needed to be installed manually. NIST SP 800-37 states that continuous monitoring results should be 
considered with respect to necessary updates to an organization’s 
security plan, security assessment report, and plan of action and 
milestones, since these documents are used to guide future risk 
management activities. The information provided by these updates helps 
to raise awareness of the current security state of the information system 
and supports the process of ongoing authorization and near real-time risk 
management. However, State did not incorporate the results of iPost continuous risk 
monitoring activities into the OpenNet security plan, security assessment 
report, and plan of action and milestones on an ongoing basis. For 
example, plans of action and milestones were not created or updated for 
guiding and monitoring the remediation of vulnerabilities deemed to be 
exceptions. 19 Thus, key information needed for tracking and resolving 
exceptions was not readily available. As a result, the department may 
limit the effectiveness of those documents in guiding future risk 
management activities. 19An exception is created when a security weakness or vulnerability cannot be resolved by 
local administrators for technical or organizational reasons beyond local control. The score 
for the identified vulnerability is transferred to the organization responsible for addressing 
the exception.</div><div id="chunk-29" class="text-chunk" /><a name="chunk-29"><sup>[29]</sup></a> State Officials Did Not 
Incorporate iPost Results 
to Update Key Security 
Documents 
 
  
 
 
 
Page 24 
GAO-11-149  State’s iPost Application  
Organizations establish controls to provide reasonable assurance that 
their data are timely, free from significant error, reliable, and complete for 
their intended use. According to Standards for Internal Control in the 
Federal Government,
20 agencies should employ a variety of control 
activities suited for information systems to ensure the accuracy and 
completeness of data contained in the system and that the data is 
available on a timely basis to allow effective monitoring of events and 
activities, and to allow prompt reaction. These controls can include 
validating data; reviewing and reconciling output to identify erroneous 
data; and reporting, investigating, and correcting erroneous data. These 
controls should be clearly documented and evaluated to ensure they are 
functioning properly. NIST SP 800-39 also states that the processes, 
procedures, and mechanisms used to support monitoring activities should 
be validated, updated, and monitored. According to the Foreign Affairs 
Manual, stakeholders, system owners, and data stewards must ensure 
the availability, completeness, and quality of department data. State has developed and implemented several controls that are intended 
to ensure the timeliness, accuracy, and completeness of iPost data. For 
example, State has employed the use of automated tools to collect 
monitoring data that are integrated into iPost. The use of automated tools 
is generally faster, more efficient, and more cost-effective than manual 
collection techniques. Automated monitoring is also less prone to human 
error.</div><div id="chunk-30" class="text-chunk" /><a name="chunk-30"><sup>[30]</sup></a> State also has used data collection schedules that support the 
frequent collection of monitoring data. For example, every Windows host 
at each iPost site is to be scanned for vulnerabilities every 7 days. The 
frequent collection of data helps to ensure its timeliness. In addition, State 
has established three scoring components—SMS Reporting, Vulnerability 
Reporting, and Security Compliance Reporting—in its risk scoring 
program to address instances when data collection tools do not correctly 
report the data required to compute a score for a component, such as 
when a host is not scanned. To illustrate, a host is assigned a score for 
the Vulnerability Reporting component if it misses two or more 
consecutive vulnerability scans (that is, the host is not scanned in 15 
days). Intended to measure the risk of the unknown according to iPost 
                                                                                                                       
20GAO, Standards for Internal Control in the Federal Government, GAO/AIMD-00.21.3.1 
(Washington, D.C.: November 1999). GAO also issued a management evaluation tool to 
assist agencies in maintaining or implementing effective internal control. See GAO, 
Internal Control Management and Evaluation Tool, GAO-01-1008G (Washington, D.C.: 
August 2001). State Has 
Implemented Controls 
Aimed at Ensuring the 
Timeliness, Accuracy, 
and Completeness of 
Data in iPost, but 
Opportunities for 
Improvement Exist 
 
  
 
 
 
Page 25 
GAO-11-149  State’s iPost Application  
documentation, this scoring method also serves as a control mechanism 
for identifying and monitoring hosts from which data were not collected in 
accordance with departmental criteria. State officials also advised that they had conducted a pilot program for 
the risk scoring program that enabled site users to (1) review the results 
of the data collections and associated scoring of the weaknesses and (2) 
report any inaccuracies they observed. State then identified solutions for 
the inaccuracies observed.</div><div id="chunk-31" class="text-chunk" /><a name="chunk-31"><sup>[31]</sup></a> Although the pilot was completed in April 
2009, State officials noted they continue to rely on iPost users to report 
missed scans and inaccurate or incomplete data observed. Notwithstanding these controls, the timeliness, accuracy, and 
completeness of iPost data were not always assured. For example, 
several instances where iPost data were not updated as frequently as 
scheduled, inconsistent, or incomplete are illustrated below.  
Frequency of updates to iPost data supports federal requirements but 
vulnerability scanning was not conducted as frequently as State 
scheduled. FISMA requires that agencies conduct periodic testing and 
evaluation of the effectiveness of information security policies, 
procedures, and practices, to be performed with a frequency based on 
risk but no less frequent than annually. According to iPost 
documentation, each host is to be scanned for vulnerabilities every 7 
days. However, a review of scanning data for 15 sites (or 120 weekly 
scans) during an 8-week period in summer 2010 revealed that only 7 
percent of the weekly scans successfully checked all Windows hosts 
at the site scanned. While 54 percent of the weekly scans 
successfully checked between 80 percent to 99 percent of the site’s 
Windows hosts, 28 percent of the scans checked less than 40 percent 
of a site’s hosts. Ten sites experienced at least one weekly scan cycle 
during which none of their hosts were scanned for vulnerabilities and 
a rescheduled scan was also not performed. According to iPost 
documentation, a host may not have been scanned because the host 
was powered off when the scan was attempted, the host’s Internet 
protocol address was not included in the range of the scan, or the 
scanning tool did not have sufficient permissions to scan the host.</div><div id="chunk-32" class="text-chunk" /><a name="chunk-32"><sup>[32]</sup></a> Although the frequency of updates to iPost data supports State’s 
efforts to satisfy FISMA’s requirement for periodic testing and 
evaluation, the updates to vulnerability information in iPost were not 
as timely as intended. As a result, iPost users may base risk 
management decisions on incomplete data. Page 26 
GAO-11-149  State’s iPost Application  
 
Data from vulnerability scans were sometimes not uploaded to iPost 
in a timely manner. State officials stated that vulnerability scanning 
results are typically presented in the iPost staging database at least 1 
day following the scan. However, the length of time it took for scan 
results to be uploaded into iPost was not consistent across all sites 
and impacted the scoring results for certain sites. The scanning 
results for the majority of 15 sites reviewed were typically presented in 
iPost at least 1 day following the scan, although it took up to 3 days 
for certain foreign and domestic sites. According to State officials, 
delays with uploading information to iPost for certain geographical 
areas around the world occurred because of the network’s 
architecture. As a result, numerous Windows hosts at 6 of the 15 sites 
reviewed received scores in iPost’s vulnerability reporting component 
for missing two consecutive vulnerability scans even though the hosts 
had been scanned. Consequently, iPost users may make risk 
management decisions based on inaccurate or incomplete data.  
Data presented in iPost about the number of hosts addressed were 
sometimes inconsistent. According to State officials, the information in 
iPost-generated reports should reflect the information displayed on 
iPost screens; however, information presented about the number of 
hosts was sometimes inconsistent.</div><div id="chunk-33" class="text-chunk" /><a name="chunk-33"><sup>[33]</sup></a> For example, the number of hosts 
that were not scanned for security compliance differed between the 
iPost reports and the site summary screens for each of the 15 sites 
reviewed. According to a State official, the summary screen displayed 
number of hosts that were not scanned over two weekly cycles 
whereas the iPost reports presented the number of hosts that were 
not scanned during the current weekly cycle but iPost did not clearly 
label the data elements accordingly. In addition, several iPost reports 
generated on the same day at one site showed a different number of 
hosts for which SMS did not report data. iPost-generated enterprise 
reports also varied in terms of the total number of hosts being 
monitored and scored, ranging from approximately 84,000 to 121,000. As a result, iPost users may base risk management decisions on 
inconsistent or inaccurate data. Several factors contributed to the conditions described above. Technical 
limitations of the data collection tools contributed to missed scans. For 
example, the diagnostic scanning tool used by State performed agentless 
network scans of specific Internet protocol address ranges, so hosts that 
are powered off during the scan or are not included in the address range 
during the scan are missed. State officials were aware of the limitations 
with the scanning tool and indicated they had taken steps to address 
them. Specifically, State acquired and was implementing a new 
 
  
 
 
 
Page 27 
GAO-11-149  State’s iPost Application  
diagnostic tool based on agent technology to collect vulnerability and 
security compliance data.</div><div id="chunk-34" class="text-chunk" /><a name="chunk-34"><sup>[34]</sup></a> Also, iPost did not retain detailed data from 
multiple cycles of scans of hosts at a site over an extended period since 
the data was overwritten by new scan results or was deleted. As a result, 
iPost users could not conduct trend analyses to determine the extent to 
which scans were successfully completed as scheduled or that data are 
accurately and consistently presented in iPost screens and reports. State 
recognized the importance of having detailed historical scan data. As 
noted earlier, a State official stated that a new policy was being 
implemented that requires detailed data be retained for two to three scans 
so that users at a site can see what changed. In addition, State had not adequately documented all of the controls in 
place for ensuring the timeliness, accuracy, and completeness of data 
and based on our review, we could not determine if all of the stated 
controls were in place or working as intended. Further, State had not 
implemented formal procedures for systematically validating data and 
reviewing and reconciling output in iPost on an ongoing basis to detect 
and correct inconsistent and incomplete data, which State officials 
confirmed were not in place. Developing, documenting, and implementing 
these procedures and controls, and ensuring that they are working as 
intended, can provide increased assurance that information displayed in 
iPost is consistent, accurate, and complete. State’s implementation of iPost has resulted in improvements to the 
department’s information security by providing extensive and timely 
information on vulnerabilities and weaknesses on Windows servers and 
workstations, while also creating an environment where officials are 
motivated to fix vulnerabilities based on department priorities. However, 
State has faced, and will continue to face, challenges in implementing 
iPost. These challenges include overcoming limitations and technical 
issues with data collection tools, identifying and notifying individuals with 
responsibility for site-level security, implementing configuration 
management, and adopting a continuous monitoring strategy for moving 
forward in incorporating additional functionality into iPost.</div><div id="chunk-35" class="text-chunk" /><a name="chunk-35"><sup>[35]</sup></a> iPost Provides Many 
Benefits, but Also 
Poses Challenges 
 
  
 
 
 
Page 28 
GAO-11-149  State’s iPost Application  
The implementation of iPost has enhanced information security at the 
department by offering a custom application with a common methodology 
for data collection, analysis, and reporting of information that security 
officers and system administrators can use to find extensive information 
on the security of Windows hosts that they are responsible for and fix 
specified vulnerabilities. For example, information in iPost allows users to: 
 
obtain a quick visual overview of compliance, vulnerability, patch, 
antivirus, and other component status for Windows hosts via the site 
summary report; 
 
access information about the status of security controls to determine 
the extent to which the controls are implemented correctly; and 
 
determine which hosts were scanned or not scanned and when this 
occurred. iPost and the risk scoring program have also facilitated the identification 
of other potential security problems since users could make connections 
between pieces of data to find possible trends or patterns. For example, 
one official responded in the survey that he was able to identify a network 
performance problem by reviewing data available on the iPost portal and 
as a result, increase the data transmission rates over the network for his 
site. In addition, since regional and enterprise managers have access to 
iPost data for sites for the region or enterprise, they have increased 
awareness of security issues at specific sites and across the enterprise, 
allowing department officials a common language with which to discuss 
vulnerabilities and make decisions regarding their mitigation. Moreover, the inclusion of a scoring approach, with associated ranking of 
sites and letter grades within iPost, has created a mechanism for the 
department chief information security officer to use in conveying to 
system administrators the department’s priorities in addressing the 
mitigation of identified vulnerabilities or implementation of particular 
patches, among other things. The scoring method has motivated security 
officers and system administrators to focus on the vulnerabilities that 
have been given the highest scores first and mitigate these weaknesses 
on affected machines. This approach also allows regional and enterprise 
officials who review the letter grades and rankings to identify sites where 
improvements need to be made. Having this capability has enabled the 
department to respond to emerging threats associated with vulnerabilities 
in commercial products that occurred over the past year. iPost implementation has also enhanced information security at State 
because having a continuous monitoring program in place provides 
Implementing iPost Has 
Helped State to Rapidly 
Identify and Fix 
Vulnerabilities on Windows 
Hosts 
 
  
 
 
 
Page 29 
GAO-11-149  State’s iPost Application  
information on weaknesses affecting Windows devices.</div><div id="chunk-36" class="text-chunk" /><a name="chunk-36"><sup>[36]</sup></a> In particular, 
controls on these devices are assessed more often than the testing and 
evaluations of controls that are performed as part of certification and 
accreditation of OpenNet every 3 years. By taking steps to implement 
continuous monitoring through iPost, State has been able to obtain 
information on vulnerabilities and weaknesses affecting tens of thousands 
of Windows devices on its OpenNet network every couple of days, 
weekly, or biweekly. Having this type of capability has enabled 
department officials to identify vulnerabilities and fix them more rapidly 
than in years prior to iPost implementation. Limitations in the capabilities of commercially available tools and 
technical issues with the tools used to collect data on vulnerabilities 
created challenges for State implementing a continuous monitoring 
program. State officials stated that when they initially began to 
conceptualize the application there were no commercial products 
available with the functionality and capabilities needed, so they developed 
iPost with the assistance of contractors. There were challenges involved 
with iPost’s development, including resolving the technical issues with 
using scanning tools and displaying the results obtained from various 
data collection tools that had different data file formats. For example, 
State officials identified the following technical issues with the data 
collection tools: 
 
Certain tools did not always check each control setting as expected, 
did not always scan hosts when scheduled, or created false positives 
that had to be analyzed and explained.  
A vendor did not consistently keep its scanning tool up to date with 
the common vulnerabilities and exposures from the National 
Vulnerability Database.  
Scanning tools of different vendors used different approaches for 
scoring groups of vulnerabilities, so when the agent software scanner 
of a new vendor was implemented, State had to curve scores so that 
the disparities did not penalize the site. Another challenge with running scans is that scanning tools do not have 
the capability to scan tens of thousands of hosts at one time without 
significant network performance degradation.</div><div id="chunk-37" class="text-chunk" /><a name="chunk-37"><sup>[37]</sup></a> Therefore, the department 
has had to establish scanning schedules so all hosts can be scanned 
over a period of time. Challenges Exist for State 
in Implementing 
Continuous Monitoring 
with iPost 
 
  
 
 
 
Page 30 
GAO-11-149  State’s iPost Application  
State officials stated they had taken steps to address these challenges by 
working with a vendor to enhance its data collection tool and selecting an 
alternate tool when appropriate. In addition, department officials stated they 
were working with other agencies and a contractor to develop additional 
capabilities that better meet their needs. Building these relationships could 
benefit the department as it moves forward in monitoring additional controls 
and developing additional capabilities in iPost. According to Standards for Internal Control in the Federal Government,
21 
authority and responsibility should be clearly assigned throughout the 
organization and clearly communicated to all employees. Responsibility 
for decision making is clearly linked to the assignment of authority, and 
individuals are held accountable accordingly. iPost generally identified the local administrator(s) for each Windows host 
who would generally have the access permissions necessary to resolve 
nonexception weaknesses on the host. However, iPost did not identify the 
individual or contact point at each site or operational unit who had site-
level responsibility for reviewing iPost site reports, monitoring the security 
state of the site’s hosts, and ensuring that the control weaknesses 
identified on all hosts at the site were resolved. In particular, there was 
confusion at the department as to who was responsible for operational 
units when this information was requested, and the information that was 
subsequently provided was inaccurate for several units. As a result, the 
department has reduced assurance that responsibility for monitoring the 
security state of and resolving known weaknesses on a site’s Windows 
hosts is clearly conveyed.</div><div id="chunk-38" class="text-chunk" /><a name="chunk-38"><sup>[38]</sup></a> In addition, departmental officials did not always notify senior managers 
at sites with low security grades of the need to fix security weaknesses. According to State officials, operational units in iPost with grades C- or 
below for 3 consecutive months are to receive warning letters indicating 
the need to improve their grades. 22 From April 2009 to March 2010, 62 out 
of 483 sites received letters noting the need for improvement; however, 6 
additional sites should have received letters but did not. In addition, 33 
                                                                                                                       
21GAO/AIMD-00-21.3.1; GAO-01-1008G. 22This information is maintained in a separate database outside of iPost to store historical 
information on site letter grades and the average host score in order to track the 
performance of sites over time. Identifying and Notifying 
Individuals with Responsibility 
for Site-Level Security 
 
  
 
 
 
Page 31 
GAO-11-149  State’s iPost Application  
sites that received at least one warning letter should have received one or 
more additional warnings for months with low grades but did not. As a 
result, senior managers may not have been fully aware of the security 
state of Windows hosts at sites they oversee. According to the Foreign Affairs Handbook, the development of new IT 
services, systems and applications, and feature and maintenance 
enhancements are to follow the guidance outlined in the Foreign Affairs 
Manual. The Foreign Affairs Manual states that configuration 
management
23 plans should be developed for IT projects and identify the 
system configuration, components, and the change control processes to 
be put in place. Effective configuration management also includes a 
disciplined process for testing configuration changes and approving 
modifications, including the development of test plan standards and 
documentation and approval of test plans, to make sure the program 
operates as intended and no unauthorized changes are introduced.</div><div id="chunk-39" class="text-chunk" /><a name="chunk-39"><sup>[39]</sup></a> State had not fully implemented configuration management for iPost. Although the department had maintained release notes on updates, 
scoring documents, and presentations on iPost, key information about the 
program and its capabilities was not fully documented. For example, there 
were no diagrams of the architecture of iPost or a configuration baseline. In addition, there was no documentation of appropriate authorization and 
approval of changes included in iPost updates. Furthermore, although 
State improved its process for testing applications and subsequent 
versions of iPost from a manual and informal testing process in April 
2010, it still lacks a written test plan and acceptance testing process with 
new releases being approved prior to release. For example, test 
procedures were not performed or documented to ensure that scripts for 
applying scoring rules matched the stated scoring methodology and that 
the scoring scripts were sufficiently tested to ensure that they fulfilled 
State’s intended use. As the department moves forward with implementation of additional 
capabilities for iPost, the need for a robust configuration management 
and testing process increases. Until such a process is fully developed, 
                                                                                                                       
23Configuration management allows an organization to develop requirements and 
implement appropriate controls to ensure requirements are met. Such controls provide 
reasonable assurance that changes to information system resources are authorized and 
systems are configured and operated securely as intended. Implementing Configuration 
Management for iPost 
 
  
 
 
 
Page 32 
GAO-11-149  State’s iPost Application  
documented, and maintained, State has reduced assurance that iPost is 
configured properly and updates or changes to the application and 
scoring rules are working as intended.</div><div id="chunk-40" class="text-chunk" /><a name="chunk-40"><sup>[40]</sup></a> According to NIST, as part of a risk management framework for federal 
information systems, a strategy for the selection of appropriate security 
controls to monitor and the frequency of monitoring should be developed 
by the information system owner and approved by the authorizing official 
and senior information security officer. Priority for selection of controls to 
monitor should be given to controls that are likely to change over time. In 
addition, the security status of the system should be reported to the 
authorizing official and other appropriate organizational officials on an 
ongoing basis in accordance with the monitoring strategy. The authorizing 
official should also review the effectiveness of deployed controls on an 
ongoing basis to determine the current risk to organizational operations 
and assets. According to the Foreign Affairs Manual, risk management 
personnel should balance the tangible and intangible cost to the 
department of applying security safeguards against the value of the 
information and the associated information system. While State has reported success with implementing iPost to provide 
ongoing monitoring of certain controls over Windows hosts on OpenNet 
and reporting the status of these controls across the enterprise to 
appropriate officials, the department faces an ongoing challenge in 
continuing this success because it does not have a documented 
continuous monitoring strategy in place. Although the department began 
continuous monitoring before applicable detailed federal guidance was 
available and selected controls to monitor based on the capabilities of 
existing data collection tools, the department has not re-evaluated the 
controls monitored to determine whether the associated risk has 
changed. In addition, although department officials reported they were 
working to implement additional controls, there was no documentation to 
indicate whether the department had weighed the associated risk and the 
tangible and intangible costs associated with implementation when 
selecting which controls they intended to monitor. Furthermore, the 
frequency of how often the security status of the Windows hosts should 
be reported to the authorizing official and other appropriate officials was 
not documented. Therefore, until the department develops, documents, 
and implements a continuous monitoring strategy, the department may 
not have sufficient assurance that it is effectively monitoring the deployed 
security controls and reporting the security status to designated officials 
with sufficient frequency.</div><div id="chunk-41" class="text-chunk" /><a name="chunk-41"><sup>[41]</sup></a> Adopting a Strategy for 
Continuous Monitoring of 
Controls 
 
  
 
 
 
Page 33 
GAO-11-149  State’s iPost Application  
Leading practices for program management, established by the Project 
Management Institute in The Standard for Program Management,
24 state 
that the information that program stakeholders need should be made 
available in a timely manner throughout the life cycle of a program. In 
addition, Standards for Internal Control in the Federal Government
25 
states that information should be communicated to management and 
others within the agency who need it. Management should also ensure 
there are adequate means of communicating with external stakeholders 
that may have a significant impact on the agency achieving its goals. A 
further ongoing challenge for the department is understanding and 
managing internal and external stakeholders’ expectations for continuous 
monitoring activities in terms of what goals and objectives can reasonably 
be achieved with iPost. These expectations include: 
 
Lowering scores in iPost always implies that risks to the individual 
sites are decreasing. With the current scoring approach used in iPost, 
lowering a score may imply that the associated risks to the site are 
being lowered as well, but there may be other reasons for the score 
being adjusted that are not related to mitigating the risk to particular 
hosts or sites. In particular, State officials have reported that  
(1) curving of the scores is performed in order to promote fairness;  
(2) exceptions are granted which shift the score from one operational 
unit to another; and (3) moving responsibility for hosts at overseas 
units to domestic units, which adjusts the scores accordingly. State 
officials should be careful in conveying to managers who make 
decisions based on scores and grades that lowering of scores in iPost 
doesn’t necessarily indicate that risks to the department are 
decreasing.  
Having continuous monitoring may replace the need for other 
assessment and authorization activities. According to NIST, a well 
designed and managed continuous monitoring program can transform 
an otherwise static and occasional security control assessment and 
risk determination process that is part of periodic assessment and 
authorization into a dynamic process that provides essential, near, 
real-term security status-related information.</div><div id="chunk-42" class="text-chunk" /><a name="chunk-42"><sup>[42]</sup></a> However, continuous 
monitoring does not replace the explicit review and acceptance of risk 
                                                                                                                       
24Project Management Institute, The Standard for Program Management, Second Edition 
(Newton Square, Pa.: 2008). 25GAO/AIMD-00-21.3.1. Managing Stakeholder 
Expectations for Continuous 
Monitoring Activities 
 
  
 
 
 
Page 34 
GAO-11-149  State’s iPost Application  
by an authorizing official on an ongoing basis as part of security 
authorization and in itself, does not provide a comprehensive, 
enterprisewide risk management approach. In addition, since 
continuous monitoring may identify risks associated with control 
weaknesses on a frequent basis, there may be instances where the 
problem cannot be fixed immediately, such as cases where State 
granted exceptions for weaknesses for periods of a year or more. There will need to be a mechanism in place for the designated 
authority to approve the associated risks from granting these 
exceptions. As the department moves forward with implementation of 
additional capabilities, it will be important to recognize the limitations 
of continuous monitoring when undertaking these efforts. State officials confirmed that managing stakeholder expectations, in 
particular external stakeholders, had been a challenge. The Chief 
Information Security Officer (CISO) stated that the department was 
attempting to address these expectations by clarifying information or 
giving presentations to external audiences, and specifically 
communicated that iPost was not intended to entirely replace all 
certification and accreditation activities. If State continues to provide 
reliable and accurate information regarding continuous monitoring 
capabilities to both internal and external stakeholders, then the 
department should be able to effectively manage stakeholder 
expectations. State’s implementation of iPost has improved visibility over information 
security at the department by providing enhanced monitoring of Windows 
hosts on the OpenNet network with nearer-to-real-time awareness of 
security vulnerabilities.</div><div id="chunk-43" class="text-chunk" /><a name="chunk-43"><sup>[43]</sup></a> As part of this effort, State’s development of a risk 
scoring program has led the way in creating a mechanism that prioritizes 
the work of system administrators to mitigate vulnerabilities; however, it 
does not incorporate all aspects of risk. Establishing a process for 
defining and prioritizing risk through a scoring mechanism is not simple 
and solutions to these issues have not yet been developed at State. Neverthless, State’s efforts to work on addressing these issues could 
continue to break new ground in improving the visibility over the state of 
information security at the department. iPost has helped IT administrators identify, monitor, and mitigate 
information security weaknesses on Windows hosts. In addition, State 
officials reported that using iPost had led them to make other security 
improvements at their sites. However, while iPost provides a useful tool 
Conclusions 
 
  
 
 
 
Page 35 
GAO-11-149  State’s iPost Application  
for identifying, monitoring, and reporting on vulnerabilities and 
weaknesses, State officials have not used iPost results to update key 
security documents which can limit the effectiveness of those documents 
in guiding future risk management activities. As part of iPost implementation, State has implemented several controls 
that are intended to help ensure timeliness, accuracy, and completeness 
of iPost data; however, vulnerability scans were not always conducted 
according to State’s schedule, and scanning results were uploaded to 
iPost in an inconsistent manner. Further, iPost data were not always 
consistent and complete. The acquisition and implementation of new data 
collection tools may help State overcome technical limitations of its 
scanning tool. Establishing robust procedures for validating data and 
reviewing and reconciling output on an ongoing basis to ensure data 
consistency, accuracy, and completeness can provide additional 
assurance to iPost users and managers who make risk management 
decisions regarding the allocation and prioritization of resources for 
security mitigation efforts at sites or across the enterprise based on iPost 
data.</div><div id="chunk-44" class="text-chunk" /><a name="chunk-44"><sup>[44]</sup></a> iPost provides several benefits in terms of providing more extensive and 
timely information on vulnerabilities, while also creating an environment 
where officials are motivated to fix vulnerabilities based on department 
priorities. Nevertheless, State faces ongoing challenges with continued 
implementation of iPost. As State implements additional capabilities and 
functionality in iPost, the need increases for the department to identify 
and notify individuals responsible for site-level security, develop 
configuration management and testing documentation, develop a 
continuous monitoring strategy, and manage and understand internal and 
external stakeholder expectations in order to ensure the continued 
success of the initiative for enhancing department information security. To improve implementation of iPost at State, we recommend that the 
Secretary of State direct the Chief Information Officer to take the following 
seven actions: 
 
Incorporate the results of iPost’s monitoring of controls into key 
security documents such as the OpenNet security plan, security 
assessment report, and plan of action and milestones.  
Document existing controls intended to ensure the timeliness, 
accuracy, and completeness of iPost data. Recommendations for 
Executive Action 
 
  
 
 
 
Page 36 
GAO-11-149  State’s iPost Application  
 
Develop, document, and implement procedures for validating data 
and reviewing and reconciling output in iPost to ensure data 
consistency, accuracy, and completeness.  
Clearly identify in iPost individuals with site-level responsibility for 
monitoring the security state and ensuring the resolution of security 
weaknesses of Windows hosts.  
Implement procedures to consistently notify senior managers at sites 
with low security grades of the need for corrective actions, in 
accordance with department criteria.  
Develop, document, and maintain an iPost configuration management 
and test process.  
Develop, document, and implement a continuous monitoring strategy 
that addresses risk, to include changing threats, vulnerabilities, 
technologies, and missions/business processes.</div><div id="chunk-45" class="text-chunk" /><a name="chunk-45"><sup>[45]</sup></a> In written comments on a draft of this report signed by the Chief Financial 
Officer for the Department of State, reproduced in appendix III, the 
department said the report was generally helpful in identifying the 
challenges State faces in implementing a continuous monitoring program 
around the world. In addition, State described metrics that it uses for 
correcting known vulnerabilities and measuring relative risks at sites. The 
department also concurred with two of our recommendations, partially 
concurred with two, and did not concur with three. Specifically, State concurred with our recommendations and indicated 
that it has or will (1) implement procedures to consistently notify senior 
managers at sites with low security grades of the need for corrective 
actions, in accordance with department criteria, and (2) develop, 
document, and implement a continuous monitoring strategy. State partially concurred with our recommendation to develop, document, 
and implement procedures for validating data and reviewing and 
reconciling output in iPost to ensure data consistency, accuracy, and 
completeness. The department stated that it had developed and 
implemented procedures for validating and testing output in iPost by 
scanning for vulnerabilities every 7 days and establishing three scoring 
components to score hosts when data collection tools do not correctly 
report the data required to compute a score. We agree and acknowledge 
in our report that the department has established these controls; however, 
the controls do not always ensure that if data is collected, it is accurate 
Agency Comments 
and Our Evaluation 
 
  
 
 
 
Page 37 
GAO-11-149  State’s iPost Application  
and complete. As mentioned in the report, we identified instances where 
iPost data was inconsistent, incomplete, or inaccurate, including the 
scoring of hosts for missed vulnerability scans when a scan had occurred. State officials make decisions about the prioritization of control weakness 
mitigation activities and allocation of resources based on information in 
iPost and so the accuracy and completeness of that information is 
important. Having procedures for validating data and reconciling the 
output in iPost will help ensure that incomplete or incorrect data is 
detected and corrected, and documenting these procedures will help 
ensure that they are consistently implemented.</div><div id="chunk-46" class="text-chunk" /><a name="chunk-46"><sup>[46]</sup></a> State also partially concurred with our recommendation that the 
department develop, document, and maintain an iPost configuration 
management and test process. The department questioned the need to 
have a diagram of the architecture of iPost and a written test plan and 
acceptance testing process, and stated that our report noted State had 
improved its process for testing versions of iPost. We have modified the 
report to provide additional context for the statement regarding State’s 
testing process in order to clarify any misunderstanding. In addition, as 
mentioned in the report, we identified areas where testing procedures 
were not performed or documented, including ensuring the scripts for 
applying the scoring rules matched the stated scoring methodology. In 
addition to lacking basic diagrams showing iPost interactions, we also 
determined that the department lacked a configuration baseline and 
documented approval process for iPost changes. Having a robust 
configuration management and testing process helps to provide 
reasonable assurance that iPost is configured properly, that updates or 
changes to the application are working as intended, and that no 
authorized changes are introduced, all of which helps to ensure the 
security and effectiveness of the continuous monitoring application. State did not concur with our recommendation for incorporating the 
results of iPost’s monitoring of controls into key security documents such 
as the OpenNet security plan, security assessment report, and plan of 
action and milestones. State did not provide a rationale for its 
nonconcurrence with our recommendation, and instead focused on 
providing additional information about the department’s use of metrics 
related to assigning risk values. As NIST guidance indicates, 
incorporating results from continuous monitoring activities into these key 
documents supports the process of ongoing authorization and near real-
time risk management. In addition, as mentioned in the report, State has 
granted exceptions for weaknesses in iPost for periods of a year or more 
but has not created or updated plans of action and milestones to guide 
 
  
 
 
 
Page 38 
GAO-11-149  State’s iPost Application  
and monitor the remediation of these exceptions.</div><div id="chunk-47" class="text-chunk" /><a name="chunk-47"><sup>[47]</sup></a> Continuous monitoring 
does not replace the explicit review and acceptance of risk by an 
authorizing official on an ongoing basis as part of security authorization. The results of iPost’s monitoring of controls, including the ongoing 
monitoring and remediation of the exceptions, needs to be documented in 
order to identify the resources and timeframes necessary for correcting 
the weaknesses. In addition, the designated authority will need to review 
these results to ensure OpenNet is operating at an acceptable level of 
risk. In addition, the department did not concur with our recommendation to 
document existing controls intended to ensure the timeliness, accuracy, 
and completeness of iPost data because it stated that it regularly 
evaluates iPost data in these areas and stated that further documentation 
was of questionable value. However, as mentioned in our report, we 
identified incomplete, inconsistent, or inaccurate data in iPost during our 
review and could not determine if all of the controls the department told 
us they implemented were actually in place or working as intended. Documenting the controls helps to provide assurance that all appropriate 
controls have been considered and can be used as a point of reference to 
periodically assess whether they are working as intended. The department also did not concur with our recommendation to clearly 
identify in iPost individuals with site-level responsibility for monitoring the 
security state and ensuring the resolution of security weaknesses of 
Windows hosts. The department noted that it failed to understand the 
necessity for individually naming those staff with site-level responsibility in 
iPost since we had surveyed State officials regarding their use of iPost. As we noted in the report, the department relies on users to report when 
inaccurate and incomplete iPost data and scoring is identified, so that it 
may be investigated and corrected as appropriate—even though there is 
no list in iPost showing who is responsible for a particular operational unit. As we discovered when we surveyed State officials, there was confusion 
at the department as to who was responsible for operational units and 
information provided to us on who was responsible was incorrect for 
several units.</div><div id="chunk-48" class="text-chunk" /><a name="chunk-48"><sup>[48]</sup></a> To clarify this issue, we have incorporated additional 
context in the report on identifying individuals with responsibility for site-
level security. Lastly, the department did not concur with our findings that the iPost risk 
scoring program does not provide a complete view of the information 
security risks to the department. Although the department’s response 
generally did not address the findings made in the report, the department 
 
  
 
 
 
Page 39 
GAO-11-149  State’s iPost Application  
did state that progress in addressing control weaknesses in iPost had led 
to an 89 percent reduction in measured cyber security risk and that it was 
impossible and impracticable to cover all areas of information security 
and security controls in NIST 800-53 as part of a continuous monitoring 
program. However, we did not state that all areas of information security 
and all controls in NIST 800-53 should be monitored as part of such a 
program. Rather, we stated that because iPost monitors only Windows 
devices and not other devices on OpenNet, addresses a select set of 
controls, and because State officials could not demonstrate the extent to 
which all components that are needed to measure risk— threats, the 
potential impacts of the threats, and the likelihood of occurrence—were 
considered when developing the scoring, that iPost does not provide a 
complete view of the information security risks to the department. Furthermore, as we mentioned in the report, the department should 
exercise care in implying that the lowering of scores in iPost means that 
risks to individual sites are decreasing as there may be other reasons for 
the score being adjusted that are not related to the mitigation of risk to 
particular hosts or sites, such as curving of scores or shifting of scores 
from one operational unit to another. While such activities may promote 
fairness, the lowering of scores may not necessarily indicate that risks to 
the department are decreasing. As agreed with your offices, unless you publicly announce the contents of 
this report earlier, we plan no further distribution until 30 days from the 
report date. At that time, we will send copies of this report to the 
Secretary of State and interested congressional committees. The report 
will also be available at no charge on the GAO Web site at 
http://www.gao.gov.</div><div id="chunk-49" class="text-chunk" /><a name="chunk-49"><sup>[49]</sup></a> Page 40 
GAO-11-149  State’s iPost Application  
If you or your staff have any questions regarding this report, please 
contact me at (202) 512-6244 or at wilshuseng@gao.gov. Contact points 
for our Offices of Congressional Relations and Public Affairs may be 
found on the last page of this report. Key contributors to this report are 
listed in appendix IV. Gregory C. Wilshusen 
Director, Information Security Issues 
 
 
Appendix I: Objectives, Scope, and 
Methodology 
 
 
 
Page 41 
GAO-11-149  State’s iPost Application  
The objectives of our review were to determine (1) the extent to which the 
Department of State (State) has identified and prioritized risk to the 
department in its risk scoring program; (2) how agency officials use iPost 
information to implement security improvements; (3) the controls for 
ensuring the timeliness, accuracy, and completeness of iPost information; 
and (4) the benefits and challenges associated with implementing iPost. To address our objectives, we conducted our review in Washington, D.C., 
where we obtained and analyzed program documentation, reports, and 
other artifacts specific to iPost, the scoring program and components, and 
data collections tools; and interviewed State officials. To address the first 
objective, we analyzed guidance from the National Institute of Standards 
and Technology (NIST) on risk management and vulnerability scoring and 
compared it to iPost risk scoring documentation to determine whether the 
department’s criteria and methodology were consistent with federal 
guidance. Where documentation on the department’s process for defining 
and prioritizing risk did not exist, we obtained information from agency 
officials in these areas where possible. We also interviewed agency 
officials from NIST to obtain information on the Common Vulnerability 
Scoring System and how agencies can use the scoring to more 
accurately reflect risk to agency environments. To address the second objective, we conducted interviews with State’s 
Chief Information Officer, Deputy Chief Information Officer for Operations, 
Chief Information Security Officer, selected Executive Directors, Regional 
Computer Security Officers, and Information Systems Officers or 
Information Systems Security Officers to obtain information on how these 
officials used iPost, in particular what information or summary reports 
they used from iPost to make decisions about security improvements and 
what types of security improvements were made. We analyzed the 
information provided by the officials to determine patterns regarding what 
information were used from iPost and what types of improvements were 
made.</div><div id="chunk-50" class="text-chunk" /><a name="chunk-50"><sup>[50]</sup></a> For the third objective, we analyzed department requirements for 
frequency of updates, accuracy, and completeness of iPost data to 
determine what controls should be in place. We obtained documentation 
and artifacts on department controls, or other mechanisms or procedures 
for each of the scoring components covered in iPost related to frequency, 
accuracy, and completeness of data and compared these to department 
requirements. Where the department lacked requirements in these areas, 
we analyzed our guidance on internal controls and assessment of 
controls for data reliability to determine what criteria should be in place to 
Appendix I: Objectives, Scope, and 
Methodology 
 
Appendix I: Objectives, Scope, and 
Methodology 
 
 
 
Page 42 
GAO-11-149  State’s iPost Application  
provide sufficient assurance of accurate, complete, and timely data. In 
areas where documentation on the department’s controls did not exist, we 
obtained information from department officials where possible. We also selected 15 units from the list of operational units to perform 
analyses to determine the frequency, accuracy, and completeness of data 
in iPost. Operational units were selected based on location (domestic or 
overseas), the number of hosts at the site, and bureau to ensure 
representation from among geographic and functional bureaus within the 
department. Frequency data obtained from iPost was tabulated to 
determine the number of hosts scanned and the dates scanned, and then 
the data was compared to the scanning schedule to determine the 
frequency in which scans occurred at the site for the time period of July 
19, 2010, through September 8, 2010. For accuracy and completeness, 
we compared detailed screens on information related to vulnerability and 
security compliance components for each of the above 15 sites with 
generated reports obtained from iPost. We also obtained raw scan data 
from State’s scanning tool for three financial center sites and compared 
that to iPost to check frequency and accuracy, however, an analysis of 
the data obtained determined it was unusable due to inconsistencies with 
how the data were reformatted when viewed. In addition, for 
completeness, we obtained detailed screen information on the 
configuration settings scanned as part of the security compliance 
component from one site and compared the scanned settings evaluated 
to the list of required settings in a Diplomatic Security mandatory security 
setting document for Windows XP.</div><div id="chunk-51" class="text-chunk" /><a name="chunk-51"><sup>[51]</sup></a> To address the fourth objective, we analyzed federal guidance on what 
activities should be taken as part of implementation of continuous 
monitoring, as well as department policies and guidance related to 
information technology management and projects and compared it to 
department activities undertaken for iPost implementation. We also 
obtained descriptions of benefits and challenges from the Chief 
Information Officer, Deputy Chief Information Officer for Operations, Chief 
Information Security Officer, selected Executive Directors, Regional 
Computer Security Officers, and Information Systems Officer or 
Information Systems Security Officers. We analyzed the information 
obtained from the department, federal guidance, and the results of our 
findings for the other objectives to identify patterns related to the benefits 
and challenges of implementation. For our second, third, and fourth objectives, we also obtained information 
through a survey of individuals at domestic and overseas sites to 
 
Appendix I: Objectives, Scope, and 
Methodology 
 
 
 
Page 43 
GAO-11-149  State’s iPost Application  
understand iPost current capabilities as of August of 2010. We surveyed 
individuals at 73 of the 491 operational units in iPost. We selected survey 
sites by reviewing the list of operational units in iPost and chose domestic 
sites from among each of the functional bureaus, and overseas sites from 
among each of the geographic bureaus to make sure there was coverage 
for each bureau and region in the sample. Sites within each functional 
and geographic bureau were selected based on the number of hosts at 
the site and the current letter grade received in order to include sites with 
varying numbers of hosts and grade scores. We developed a survey 
instrument to gather information from domestic and overseas department 
officials on how they used iPost at their location, whether they had 
experienced problems with using data collection tools, and what benefits 
and challenges they had experienced with implementation between 
August 1, 2009, and August 30, 2010. Our final sample included 73 sites 
(36 overseas and 37 domestic). The sample of sites we surveyed was not 
a representative sample and the results from our survey cannot be 
generalized to apply to any other sites outside those sampled.</div><div id="chunk-52" class="text-chunk" /><a name="chunk-52"><sup>[52]</sup></a> However, 
the interviews and survey information provided illustrative examples of the 
perspectives of various individuals about iPost’s current and future 
capabilities. We identified a specific respondent at each site by either 
reviewing the contact list on State’s Web site or asking State officials. This person was the Information Management Officer, Information 
System Officer, System Administrator, or Information System Security 
Officer, or the acting or assistant official in one of these positions at a 
given site. To minimize errors that might occur from respondents interpreting our 
questions differently from our intended purpose, we pretested the 
questionnaire by phone with State officials who were in positions similar 
to the respondents who would complete our actual survey during four 
separate sessions. During these pretests, we asked the officials to 
complete the questionnaire as we listened to the process. We then 
interviewed the respondents to check whether (1) the questions were 
clear and unambiguous, (2) the terms used were precise, (3) the 
questionnaire was unbiased, and (4) the questionnaire did not place an 
undue burden on the officials completing it. We also submitted the 
questionnaire for review by a GAO survey methodology expert. We 
modified the questions based on feedback from the pretests and review, 
as appropriate. Overall, of the 73 sampled sites, 40 returned completed questionnaires 
and 2 of the nonresponding sites were ineligible because they had been 
consolidated into other sites, leading to a final response rate of 57.1 
 
Appendix I: Objectives, Scope, and 
Methodology 
 
 
 
Page 44 
GAO-11-149  State’s iPost Application  
percent; however, not all respondents provided answers to every 
question. Two of the sites answered about their own site and other sites 
under their supervision; each of these was treated as a single data point 
(i.e., site) in statistical analyses.</div><div id="chunk-53" class="text-chunk" /><a name="chunk-53"><sup>[53]</sup></a> We reviewed all questionnaire 
responses, and followed up by phone and e-mail to clarify the responses 
as appropriate. The practical difficulties of conducting any survey may introduce 
nonsampling errors. For example, differences in how a particular question 
is interpreted, the sources of information available to respondents, or the 
types of respondents who do not respond to a question can introduce 
errors into the survey results. We included steps in both the data 
collection and data analysis stages to minimize such nonsampling errors. We examined the survey results and performed computer analyses to 
identify inconsistencies and other indications of error, and addressed 
such issues as necessary. An independent analyst checked the accuracy 
of all computer analyses to minimize the likelihood of errors in data 
processing. In addition, GAO analysts answered respondent questions 
and resolved difficulties respondents had answering our questions. We 
analyzed responses to closed-ended questions by counting the response 
for all sites and for overseas and domestic sites separately. For questions 
that asked respondents to provide a narrative answer, we compiled the 
open answers in one document that was analyzed and used as examples 
in the report. We conducted this performance audit from March 2010 to July 2011 in 
accordance with generally accepted government auditing standards.</div><div id="chunk-54" class="text-chunk" /><a name="chunk-54"><sup>[54]</sup></a> Those standards require that we plan and perform the audit to obtain 
sufficient, appropriate evidence to provide a reasonable basis for our 
findings and conclusions based on our audit objectives. We believe that 
the evidence obtained provides a reasonable basis for our findings and 
conclusions based on our audit objectives. Appendix II: Examples of Key iPost Screens 
and Reports 
 
 
 
Page 45 
GAO-11-149  State’s iPost Application  
A selection of key iPost screens and reports for sites are described 
below. The screen provides summary information on the site including: site’s 
grade; host summary statistics by category which provides a graphical 
representation of the number of hosts that are compliant or not compliant; 
Active Directory (AD) account information for users and computers; and 
network activity at the site (see fig. 6). Figure 6: Example of an iPost Dashboard Site Summary Screen 
 
The site risk score summary screen provides a summary of the site’s risk 
score summary, including the site’s grade, average risk score, and total 
risk score. A summary table shows the total risk score broken down by 
category. A graphical presentation of the risk score by component 
highlights components with high risk scores (see fig. 7). Appendix II: Examples of Key iPost Screens 
and Reports 
Site summary screen 
Site risk score summary screen 
Source: State. Appendix II: Examples of Key iPost Screens 
and Reports 
 
 
 
Page 46 
GAO-11-149  State’s iPost Application  
Figure 7: Example of an iPost Site Risk Score Summary Screen 
 
Detailed component screens provide breakdowns of the scoring results 
for each host.</div><div id="chunk-55" class="text-chunk" /><a name="chunk-55"><sup>[55]</sup></a> For example, the detailed security compliance screen (see 
fig. 8) identifies each host, the type of host, the date of the last security 
compliance scan, and the total risk score assigned the host. Users can 
select the details option to see more specific information on the security 
settings that failed compliance and the associated score that was 
assigned. Figure 8: Example of an iPost Detailed Security Compliance Component Screen 
 
Detailed component screens 
Source: State. Source: State. Appendix II: Examples of Key iPost Screens 
and Reports 
 
 
 
Page 47 
GAO-11-149  State’s iPost Application  
The risk score advisory report provides a summary of all the scoring 
issues for the site and summary advice on how to improve the site’s 
score. Summary information includes the site’s grade, average risk score, 
and total risk score. A graphical presentation of the risk score by 
component highlights components with high risk scores (see fig. 9). Figure 9: Example of an iPost Risk Score Advisory Report 
 
 
Risk score advisory report 
Source: State.</div><div id="chunk-56" class="text-chunk" /><a name="chunk-56"><sup>[56]</sup></a> Appendix III: Comments from the Department 
of State 
 
 
 
Page 48 
GAO-11-149  State’s iPost Application  
 
 
Appendix III: Comments from the 
Department of State 
 
Appendix III: Comments from the Department 
of State 
 
 
 
Page 49 
GAO-11-149  State’s iPost Application  
 
 
 
Appendix III: Comments from the Department 
of State 
 
 
 
Page 50 
GAO-11-149  State’s iPost Application  
 
 
 
Appendix III: Comments from the Department 
of State 
 
 
 
Page 51 
GAO-11-149  State’s iPost Application  
 
 
 
Appendix III: Comments from the Department 
of State 
 
 
 
Page 52 
GAO-11-149  State’s iPost Application  
 
 
 
Appendix III: Comments from the Department 
of State 
 
 
 
Page 53 
GAO-11-149  State’s iPost Application  
 
 
 
Appendix III: Comments from the Department 
of State 
 
 
 
Page 54 
GAO-11-149  State’s iPost Application  
 
 
 
Appendix III: Comments from the Department 
of State 
 
 
 
Page 55 
GAO-11-149  State’s iPost Application  
 
 
 
Appendix III: Comments from the Department 
of State 
 
 
 
Page 56 
GAO-11-149  State’s iPost Application  
 
 
 
Appendix III: Comments from the Department 
of State 
 
 
 
Page 57 
GAO-11-149  State’s iPost Application  
 
 
Appendix IV: GAO Contact and Staff 
Acknowledgments 
 
 
 
Page 58 
GAO-11-149  State’s iPost Application  
Gregory C. Wilshusen, (202) 512-6244 or wilshuseng@gao.gov 
 
In addition to the individual named above, Ed Alexander and William 
Wadsworth (Assistant Directors), Carl Barden, Neil Doherty, Rebecca 
Eyler, Justin Fisher, Valerie Hopkins, Tammi Kalugdan, Linda 
Kochersberger, Karl Seifert, Michael Silver, Eugene Stevens, and Henry 
Sutanto made key contributions to this report. Appendix IV: GAO Contact and Staff 
Acknowledgments 
GAO Contact 
Staff 
Acknowledgments 
(311046)
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The Government Accountability Office, the audit, evaluation, and 
investigative arm of Congress, exists to support Congress in meeting its 
constitutional responsibilities and to help improve the performance and 
accountability of the federal government for the American people. GAO 
examines the use of public funds; evaluates federal programs and 
policies; and provides analyses, recommendations, and other assistance 
to help Congress make informed oversight, policy, and funding decisions. GAO’s commitment to good government is reflected in its core values of 
accountability, integrity, and reliability. The fastest and easiest way to obtain copies of GAO documents at no 
cost is through GAO’s Web site (www.gao.gov). Each weekday afternoon, 
GAO posts on its Web site newly released reports, testimony, and 
correspondence. To have GAO e-mail you a list of newly posted products, 
go to www.gao.gov and select “E-mail Updates.” 
The price of each GAO publication reflects GAO’s actual cost of 
production and distribution and depends on the number of pages in the 
publication and whether the publication is printed in color or black and 
white. Pricing and ordering information is posted on GAO’s Web site, 
http://www.gao.gov/ordering.htm. Place orders by calling (202) 512-6000, toll free (866) 801-7077, or  
TDD (202) 512-2537. Orders may be paid for using American Express, Discover Card, 
MasterCard, Visa, check, or money order.</div><div id="chunk-57" class="text-chunk" /><a name="chunk-57"><sup>[57]</sup></a> Call for additional information. Contact: 
Web site: www.gao.gov/fraudnet/fraudnet.htm 
E-mail: fraudnet@gao.gov 
Automated answering system: (800) 424-5454 or (202) 512-7470 
Ralph Dawn, Managing Director, dawnr@gao.gov, (202) 512-4400 
U.S. Government Accountability Office, 441 G Street NW, Room 7125 
Washington, DC 20548 
Chuck Young, Managing Director, youngc1@gao.gov, (202) 512-4800 
U.S. Government Accountability Office, 441 G Street NW, Room 7149  
Washington, DC 20548 
 
GAO’s Mission 
Obtaining Copies of 
GAO Reports and 
Testimony 
Order by Phone 
To Report Fraud, 
Waste, and Abuse in 
Federal Programs 
Congressional 
Relations 
Public Affairs 
Please Print on Recycled Paper</div></div>
                </div>
            </div>
        </div>
        <br /><br />

        
                <div class="accordion">
                    <div class="accordion-item">
                        <button class="accordion-header">Overall Summary</button>
                        <div class="accordion-content"><h3>Document Summary</h3>

<h4>Primary Standards, Guidelines, and Requirements:</h4>

<ol>
<li><p><strong>Federal Information Security Management Act of 2002 (FISMA)</strong></p>

<ul>
<li>Ensures effectiveness of security controls over federal operations and assets.</li>
</ul></li>
<li><p><strong>National Institute of Standards and Technology (NIST) Guidance</strong></p>

<ul>
<li>Emphasizes risk management, vulnerability scoring, and continuous monitoring.</li>
</ul></li>
<li><p><strong>NIST Special Publication 800-37</strong></p>

<ul>
<li>Describes a risk management framework for federal information systems.</li>
</ul></li>
<li><p><strong>NIST Special Publication 800-137 (Draft)</strong></p>

<ul>
<li>Outlines continuous monitoring for federal information systems and organizations.</li>
</ul></li>
<li><p><strong>Department of Homeland Security (DHS) CAESARS Reference Architecture Report</strong></p>

<ul>
<li>Provides a framework for continuous asset evaluation, situational awareness, and risk scoring.</li>
</ul></li>
</ol>

<h4>Sections Highlighting Key Areas:</h4>

<p><strong>Security:</strong>
1. <strong>FISMA Requirements</strong>
   - Protects data based on risk and magnitude of potential harm.</p>

<ol start="2">
<li><p><strong>NIST SP 800-37</strong></p>

<ul>
<li>Involves ongoing risk management and information security controls.</li>
</ul></li>
<li><p><strong>NIST SP 800-137 (Draft)</strong></p>

<ul>
<li>Supports near real-time security status awareness.</li>
</ul></li>
</ol>

<p><strong>Accessibility:</strong>
1. <strong>Automated Monitoring Tools</strong>
   - Facilitates frequent data collection for updated information.</p>

<ol start="2">
<li><strong>Resource Links in iPost</strong>
<ul>
<li>Provides users with tools to address vulnerabilities, such as patch management and configuration guides.</li>
</ul></li>
</ol>

<p><strong>User Experience:</strong>
1. <strong>iPost Dashboard Interface</strong>
   - Offers summary and detailed monitoring data for users to review and act upon.</p>

<ol start="2">
<li><strong>Survey Feedback</strong>
<ul>
<li>Users rely on iPost data to prioritize and fix vulnerabilities.</li>
</ul></li>
</ol>

<p><strong>Compliance:</strong>
1. <strong>Scoring Program Coverage</strong>
   - Targets Windows hosts but acknowledges the need for broader coverage in future iterations.</p>

<ol start="2">
<li><strong>NIST SP 800-53 Compliance</strong>
<ul>
<li>Identifies limitations in covering all information security controls. </li>
</ul></li>
</ol>

<p>These summarized points should give you a clear understanding of the essential guidelines and requirements present in the original Government Standards Document, focusing on security, accessibility, user experience, and compliance.</p>
</div>
                    </div>
                </div>
                <br /><br />
            
                <div class="accordion">
                    <div class="accordion-item">
                        <button class="accordion-header">Punchline Summary</button>
                        <div class="accordion-content"><h3>Purpose in a Single Sentence:</h3>

<p>"This document evaluates how well a government department's cybersecurity tool monitors IT risks, pinpoints weaknesses, and suggests improvements."</p>

<h3>Punchline Summaries for Different Perspectives:</h3>

<ul>
<li><p><strong>DevOps Technician:</strong>
<strong>"Understand the need to maintain and improve a system that continuously scans and fixes security vulnerabilities in our IT setup."</strong></p></li>
<li><p><strong>Designer Specializing in Human-Centered Design:</strong>
<strong>"Ensure the cybersecurity tool's interface is intuitive so that users can easily identify and resolve IT security issues."</strong></p></li>
<li><p><strong>Developer Specializing in Drupal:</strong>
<strong>"Implement backend logic that integrates seamlessly with the tool to automatically detect and prioritize cybersecurity threats."</strong></p></li>
<li><p><strong>Project Manager:</strong>
<strong>"Organize and direct efforts to ensure the cybersecurity tool's improvements are tracked, executed, and communicated effectively."</strong></p></li>
</ul>
</div>
                    </div>
                </div>
                <br /><br />
            
                <div class="accordion">
                    <div class="accordion-item">
                        <button class="accordion-header">Actions.Devops Summary</button>
                        <div class="accordion-content"><p>Sure! Here is a minimal list of actions that a DevOps technician should take to ensure compliance with government standards for a web application:</p>

<ol>
<li><p><strong>Implement Security Controls</strong>:</p>

<ul>
<li>Configure firewalls and security groups for strict access control.</li>
<li>Use role-based access control (RBAC) for user permissions.</li>
<li>Employ strong password policies and ensure regular updates.</li>
</ul></li>
<li><p><strong>Continuous Monitoring</strong>:</p>

<ul>
<li>Set up automated monitoring tools to track system performance and security events.</li>
<li>Ensure vulnerability scanning is performed regularly (e.g., weekly).</li>
<li>Monitor anti-virus signature updates and compliance.</li>
</ul></li>
<li><p><strong>Data Encryption</strong>:</p>

<ul>
<li>Encrypt data at rest and in transit using approved cryptographic methods.</li>
<li>Ensure databases and storage devices implement encryption.</li>
</ul></li>
<li><p><strong>Patch Management</strong>:</p>

<ul>
<li>Implement an automated patch management system to ensure timely updates.</li>
<li>Regularly audit and apply software patches for all components (OS, libraries, applications).</li>
</ul></li>
<li><p><strong>Regular Backups</strong>:</p>

<ul>
<li>Configure automated backups of all critical data.</li>
<li>Test backups periodically to ensure they are recoverable.</li>
</ul></li>
<li><p><strong>Configuration Management</strong>:</p>

<ul>
<li>Maintain version-controlled infrastructure as code (IaC) repositories.</li>
<li>Use configuration management tools for consistent environment setup (e.g., Ansible, Terraform).</li>
</ul></li>
<li><p><strong>Incident Response Preparedness</strong>:</p>

<ul>
<li>Establish and document incident response procedures.</li>
<li>Conduct regular drills and training for incident management.</li>
</ul></li>
<li><p><strong>Logging and Auditing</strong>:</p>

<ul>
<li>Enable detailed logging of system and application activity.</li>
<li>Set up centralized logging and audit logs for compliance review.</li>
</ul></li>
<li><p><strong>Network Security</strong>:</p>

<ul>
<li>Implement secure network segregation (e.g., VPCs, subnets).</li>
<li>Configure load balancers with security features like DDoS protection.</li>
</ul></li>
<li><p><strong>Compliance Checks and Reporting</strong>:</p>

<ul>
<li>Conduct regular compliance checks against government standards.</li>
<li>Generate and review compliance reports periodically.</li>
</ul></li>
</ol>

<p>By implementing these actions, a DevOps technician can help ensure the web application's infrastructure meets the necessary government standards for security and compliance.</p>
</div>
                    </div>
                </div>
                <br /><br />
            
                <div class="accordion">
                    <div class="accordion-item">
                        <button class="accordion-header">Actions.Designer Summary</button>
                        <div class="accordion-content"><p>For a web application, here is a minimal list of actions that a Designer specializing in Human-Centered Design (HCD) and responsible for User Experience (UX) and User Interface (UI) could take to show compliance with standards, specifically focusing on the security elements highlighted in the Government Standards Document:</p>

<ol>
<li><p><strong>Incorporate Security by Design Principles:</strong></p>

<ul>
<li>Ensure the design minimizes risks such as clickjacking, SQL injection, and Cross-Site Scripting (XSS) by incorporating secure coding practices.</li>
</ul></li>
<li><p><strong>User Authentication and Authorization:</strong></p>

<ul>
<li>Design user interfaces that support secure login mechanisms, including multi-factor authentication (MFA).</li>
<li>Ensure roles and permissions are clearly defined and access controls are visible in the UI.</li>
</ul></li>
<li><p><strong>Data Privacy and User Consent:</strong></p>

<ul>
<li>Implement clear and accessible privacy policies.</li>
<li>Design consent forms for data collection, making sure to document the user’s explicit consent.</li>
</ul></li>
<li><p><strong>Session Management:</strong></p>

<ul>
<li>Ensure session handling (e.g., time-outs and logouts) is communicated and appropriately designed for user awareness.</li>
</ul></li>
<li><p><strong>Error Handling:</strong></p>

<ul>
<li>Design error responses that do not reveal system information. User-friendly error messages should guide users without exposing technical details.</li>
</ul></li>
<li><p><strong>Secure Data Input:</strong></p>

<ul>
<li>Design input fields to validate and sanitize user inputs. Highlight requirements for secure password entry (e.g., complexity rules).</li>
</ul></li>
<li><p><strong>Accessibility and Usability:</strong></p>

<ul>
<li>Ensure the application follows WCAG 2.1 standards to make it accessible to all users, including those with disabilities.</li>
<li>Perform usability testing to confirm ease of use does not compromise security features.</li>
</ul></li>
<li><p><strong>User Education:</strong></p>

<ul>
<li>Integrate educational components within the UI to inform users about secure practices and the importance of security measures applied in the application.</li>
</ul></li>
<li><p><strong>UI for Continuous Monitoring Alerts:</strong></p>

<ul>
<li>Design dashboards or notification systems that can display security-related alerts for continuous monitoring.</li>
</ul></li>
<li><p><strong>Feedback and Improvement Mechanisms:</strong></p>

<ul>
<li>Design feedback forms to collect user feedback on security features, ensuring users can easily report security issues.</li>
</ul></li>
<li><p><strong>Adherence to Design Standards:</strong></p>

<ul>
<li>Use standardized design frameworks and guidelines (e.g., NIST SP 800-53) to ensure compliance with recognized security requirements.</li>
</ul></li>
<li><p><strong>Risk Communication:</strong></p>

<ul>
<li>Clearly communicate risk levels and security statuses through intuitive UI elements like color-coded warnings and informative dashboards.</li>
</ul></li>
</ol>

<p>By focusing on these actions, a Designer can actively contribute to the security and compliance of a web application while ensuring a user-centered design approach.</p>
</div>
                    </div>
                </div>
                <br /><br />
            
                <div class="accordion">
                    <div class="accordion-item">
                        <button class="accordion-header">Actions.Developer Summary</button>
                        <div class="accordion-content"><p>Certainly! Here’s a minimal list of actions that a Drupal developer can take to show compliance with the given government standards related to continuous monitoring and information security, as inferred from the provided document:</p>

<ol>
<li><strong>Regular Software Updates</strong>: Ensure Drupal core and all modules are up-to-date.</li>
<li><strong>Secure Configuration</strong>: Apply required security settings and harden the server configuration for Drupal.</li>
<li><strong>Authentication Controls</strong>: Implement strong password policies, enable multi-factor authentication (MFA), and ensure user role permissions are correctly set.</li>
<li><strong>Patch Management</strong>: Regularly apply security patches for Drupal and all third-party libraries.</li>
<li><strong>Vulnerability Scanning</strong>: Integrate a vulnerability scanning tool to perform periodic checks of the Drupal site.</li>
<li><strong>Logging &amp; Monitoring</strong>: Enable and configure comprehensive logging within Drupal and set up alerts for suspicious activities.</li>
<li><strong>Data Encryption</strong>: Encrypt sensitive data both in transit and at rest.</li>
<li><strong>Backup Procedures</strong>: Implement and regularly test backup and restore procedures.</li>
<li><strong>Error Handling</strong>: Ensure all errors are properly caught, logged, and do not expose sensitive information.</li>
<li><strong>Access Controls</strong>: Regularly review and update access controls based on the principle of least privilege.</li>
<li><strong>Security Review</strong>: Perform regular security audits and reviews of the Drupal implementation.</li>
<li><strong>Automated Tests</strong>: Write and run automated tests to verify that security controls are functioning as intended.</li>
<li><strong>Environment Isolation</strong>: Separate development, staging, and production environments to reduce risk.</li>
<li><strong>Use of Secure Channels</strong>: Ensure all data exchanges use secure channels (e.g., HTTPS).</li>
</ol>

<p>By consistently following these actions, the Drupal developer can help ensure the web application remains compliant with relevant information security standards.</p>
</div>
                    </div>
                </div>
                <br /><br />
            
                <div class="accordion">
                    <div class="accordion-item">
                        <button class="accordion-header">Actions.Project Manager Summary</button>
                        <div class="accordion-content"><p>Here is a minimal list of actions for a project manager to ensure compliance with government standards for a web application:</p>

<ol>
<li><p><strong>Assess Risk</strong>:</p>

<ul>
<li>Implement a risk scoring system.</li>
<li>Prioritize vulnerabilities based on risk levels.</li>
</ul></li>
<li><p><strong>Continuous Monitoring</strong>:</p>

<ul>
<li>Utilize automated monitoring tools.</li>
<li>Schedule regular scans (e.g., weekly) for vulnerabilities.</li>
<li>Ensure monitoring data are updated timely and accurately.</li>
</ul></li>
<li><p><strong>Data Accuracy and Completeness</strong>:</p>

<ul>
<li>Validate and reconcile scan data regularly.</li>
<li>Investigate and correct any inconsistencies or inaccuracies.</li>
</ul></li>
<li><p><strong>Security Controls Implementation</strong>:</p>

<ul>
<li>Address specified security controls (e.g., patch management, antivirus updates).</li>
<li>Implement a baseline security configuration for all systems.</li>
</ul></li>
<li><p><strong>Incident Response</strong>:</p>

<ul>
<li>Establish a process for reporting and responding to security incidents.</li>
</ul></li>
<li><p><strong>User Accountability</strong>:</p>

<ul>
<li>Assign responsibility to specific individuals for site-level security.</li>
<li>Ensure users report inaccurate or incomplete data.</li>
</ul></li>
<li><p><strong>Configuration Management</strong>:</p>

<ul>
<li>Develop and document configuration management plans.</li>
<li>Maintain a configuration baseline for the system.</li>
</ul></li>
<li><p><strong>Documentation and Reporting</strong>:</p>

<ul>
<li>Keep key security documents updated (e.g., security plan, assessment report, plan of action and milestones).</li>
<li>Notify senior managers of low security grades and require corrective actions.</li>
</ul></li>
<li><p><strong>Stakeholder Communication</strong>:</p>

<ul>
<li>Regularly update stakeholders on the security status and any necessary actions.</li>
<li>Clarify the purpose and limitations of continuous monitoring to stakeholders.</li>
</ul></li>
<li><p><strong>Compliance Verification</strong>:</p>

<ul>
<li>Conduct regular internal reviews and audits to ensure all standards are being met.</li>
<li>Document all compliance efforts and maintain records for audits.</li>
</ul></li>
</ol>

<p>This list ensures that the project is organized to meet the necessary standards while tracking and guiding tasks effectively.</p>
</div>
                    </div>
                </div>
                <br /><br />
            

    </body>
    </html>